{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc05f0ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.63\n",
      "Precision: 98.63\n",
      "Recall: 98.63\n",
      "F1-Score: 98.63\n",
      "Overall TP: 98.58\n",
      "Overall TN: 97.86\n",
      "Overall FP: 0.71\n",
      "Overall FN: 0.71\n"
     ]
    }
   ],
   "source": [
    "#XGB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-All-Together-Update-Single-26-8-24.csv',\n",
    "    '2-All-Together-Update-Single-26-8-24.csv',\n",
    "    '3-All-Together-Update-Single-26-8-24.csv',\n",
    "    '4-All-Together-Update-Single-26-8-24.csv',\n",
    "    '5-All-Together-Update-Single-26-8-24.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "df['Protocol'] = label_encoder.fit_transform(df['Protocol'])\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Step 1: Split the dataset into 80% training and 20% unseen testing\n",
    "X_train, X_unseen_test, y_train, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Further split the 80% training set into 80% training and 20% validation\n",
    "X_train_split, X_validation_split, y_train_split, y_validation_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the XGBoost model on the 80% of the training split\n",
    "model = XGBClassifier(random_state=42)\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Step 4: Evaluate the model on the 20% validation split\n",
    "y_pred_validation = model.predict(X_validation_split)\n",
    "\n",
    "# Calculate validation metrics\n",
    "accuracy_validation = accuracy_score(y_validation_split, y_pred_validation)\n",
    "precision_validation = precision_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "recall_validation = recall_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "f1_validation = f1_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "\n",
    "\n",
    "y_pred_unseen = model.predict(X_unseen_test)\n",
    "\n",
    "# Calculate unseen test metrics\n",
    "accuracy_unseen = accuracy_score(y_unseen_test, y_pred_unseen)\n",
    "precision_unseen = precision_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "recall_unseen = recall_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "f1_unseen = f1_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "# Output unseen test results\n",
    "print(f\"Accuracy: {accuracy_unseen:.2f}\")\n",
    "print(f\"Precision: {precision_unseen:.2f}\")\n",
    "print(f\"Recall: {recall_unseen:.2f}\")\n",
    "print(f\"F1-Score: {f1_unseen:.2f}\")\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_unseen_test, y_pred_unseen)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "605b3da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.66\n",
      "Precision: 98.66\n",
      "Recall: 98.66\n",
      "F1-Score: 98.66\n",
      "Overall TP: 98.73\n",
      "Overall TN: 98.10\n",
      "Overall FP: 0.51\n",
      "Overall FN: 0.51\n"
     ]
    }
   ],
   "source": [
    "#XGB-B\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-Update-3-All_together-Bi.csv',\n",
    "    '2-Update-3-All_together-Bi.csv',\n",
    "    '3-Update-3-All_together-Bi.csv' \n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "df['Protocol'] = label_encoder.fit_transform(df['Protocol'])\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Step 1: Split the dataset into 80% training and 20% unseen testing\n",
    "X_train, X_unseen_test, y_train, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Further split the 80% training set into 80% training and 20% validation\n",
    "X_train_split, X_validation_split, y_train_split, y_validation_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the XGBoost model on the 80% of the training split\n",
    "model = XGBClassifier(random_state=42)\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Step 4: Evaluate the model on the 20% validation split\n",
    "y_pred_validation = model.predict(X_validation_split)\n",
    "\n",
    "# Calculate validation metrics\n",
    "accuracy_validation = accuracy_score(y_validation_split, y_pred_validation)\n",
    "precision_validation = precision_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "recall_validation = recall_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "f1_validation = f1_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "\n",
    "\n",
    "y_pred_unseen = model.predict(X_unseen_test)\n",
    "\n",
    "# Calculate unseen test metrics\n",
    "accuracy_unseen = accuracy_score(y_unseen_test, y_pred_unseen)\n",
    "precision_unseen = precision_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "recall_unseen = recall_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "f1_unseen = f1_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "# Output unseen test results\n",
    "print(f\"Accuracy: {accuracy_unseen:.2f}\")\n",
    "print(f\"Precision: {precision_unseen:.2f}\")\n",
    "print(f\"Recall: {recall_unseen:.2f}\")\n",
    "print(f\"F1-Score: {f1_unseen:.2f}\")\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_unseen_test, y_pred_unseen)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb3e5073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.51\n",
      "Precision: 98.51\n",
      "Recall: 98.50\n",
      "F1-Score: 98.50\n",
      "Overall TP: 98.44\n",
      "Overall TN: 97.75\n",
      "Overall FP: 0.75\n",
      "Overall FN: 0.75\n"
     ]
    }
   ],
   "source": [
    "#LGB-A\n",
    "#!pip install lightgbm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import LGBMClassifier  # Import LightGBM\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "          # '1-Update-3-All_together-Bi.csv',\n",
    "           #'2-Update-3-All_together-Bi.csv',\n",
    "        # '3-Update-3-All_together-Bi.csv'\n",
    "      '1-All-Together-Update-Single-26-8-24.csv',\n",
    "      '2-All-Together-Update-Single-26-8-24.csv',\n",
    "      '3-All-Together-Update-Single-26-8-24.csv',\n",
    "      '4-All-Together-Update-Single-26-8-24.csv',\n",
    "       '5-All-Together-Update-Single-26-8-24.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "df['Protocol'] = label_encoder.fit_transform(df['Protocol'])\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "df = df.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Step 1: Split the dataset into 80% training and 20% unseen testing\n",
    "X_train, X_unseen_test, y_train, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Further split the 80% training set into 80% training and 20% validation\n",
    "X_train_split, X_validation_split, y_train_split, y_validation_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the LightGBM model on the 80% of the training split\n",
    "model = LGBMClassifier(random_state=42)\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Step 4: Evaluate the model on the 20% validation split\n",
    "y_pred_validation = model.predict(X_validation_split)\n",
    "accuracy_validation = accuracy_score(y_validation_split, y_pred_validation)\n",
    "precision_validation = precision_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "recall_validation = recall_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "f1_validation = f1_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model on the 20% unseen testing data\n",
    "y_pred_unseen = model.predict(X_unseen_test)\n",
    "\n",
    "# Calculate metrics for unseen test data\n",
    "accuracy_unseen = accuracy_score(y_unseen_test, y_pred_unseen)\n",
    "precision_unseen = precision_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "recall_unseen = recall_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "f1_unseen = f1_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "\n",
    "# Output results for unseen test data\n",
    "print(f\"Accuracy: {accuracy_unseen:.2f}\")\n",
    "print(f\"Precision: {precision_unseen:.2f}\")\n",
    "print(f\"Recall: {recall_unseen:.2f}\")\n",
    "print(f\"F1-Score: {f1_unseen:.2f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_unseen_test, y_pred_unseen)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08a000d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.57\n",
      "Precision: 98.57\n",
      "Recall: 98.57\n",
      "F1-Score: 98.56\n",
      "Overall TP: 98.60\n",
      "Overall TN: 97.95\n",
      "Overall FP: 0.52\n",
      "Overall FN: 0.52\n"
     ]
    }
   ],
   "source": [
    "#LGB-B\n",
    "#!pip install lightgbm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lightgbm import LGBMClassifier  # Import LightGBM\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "           '1-Update-3-All_together-Bi.csv',\n",
    "           '2-Update-3-All_together-Bi.csv',\n",
    "        '3-Update-3-All_together-Bi.csv'\n",
    "      \n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "df['Protocol'] = label_encoder.fit_transform(df['Protocol'])\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Drop the unnecessary columns\n",
    "df = df.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Step 1: Split the dataset into 80% training and 20% unseen testing\n",
    "X_train, X_unseen_test, y_train, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Further split the 80% training set into 80% training and 20% validation\n",
    "X_train_split, X_validation_split, y_train_split, y_validation_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the LightGBM model on the 80% of the training split\n",
    "model = LGBMClassifier(random_state=42)\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Step 4: Evaluate the model on the 20% validation split\n",
    "y_pred_validation = model.predict(X_validation_split)\n",
    "accuracy_validation = accuracy_score(y_validation_split, y_pred_validation)\n",
    "precision_validation = precision_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "recall_validation = recall_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "f1_validation = f1_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model on the 20% unseen testing data\n",
    "y_pred_unseen = model.predict(X_unseen_test)\n",
    "\n",
    "# Calculate metrics for unseen test data\n",
    "accuracy_unseen = accuracy_score(y_unseen_test, y_pred_unseen)\n",
    "precision_unseen = precision_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "recall_unseen = recall_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "f1_unseen = f1_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "\n",
    "# Output results for unseen test data\n",
    "print(f\"Accuracy: {accuracy_unseen:.2f}\")\n",
    "print(f\"Precision: {precision_unseen:.2f}\")\n",
    "print(f\"Recall: {recall_unseen:.2f}\")\n",
    "print(f\"F1-Score: {f1_unseen:.2f}\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_unseen_test, y_pred_unseen)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb37df8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.22\n",
      "Precision: 98.23\n",
      "Recall: 98.22\n",
      "F1-Score: 98.22\n",
      "Overall TP: 98.17\n",
      "Overall TN: 97.47\n",
      "Overall FP: 0.91\n",
      "Overall FN: 0.91\n"
     ]
    }
   ],
   "source": [
    "#CatBoost-A\n",
    "#!pip install catboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier  # Import CatBoostClassifier from CatBoost\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "   '1-All-Together-Update-Single-26-8-24.csv',\n",
    "      '2-All-Together-Update-Single-26-8-24.csv',\n",
    "      '3-All-Together-Update-Single-26-8-24.csv',\n",
    "      '4-All-Together-Update-Single-26-8-24.csv',\n",
    "       '5-All-Together-Update-Single-26-8-24.csv' \n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "df['Protocol'] = label_encoder.fit_transform(df['Protocol'])\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Step 1: Split the dataset into 80% training and 20% unseen testing\n",
    "X_train, X_unseen_test, y_train, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Further split the 80% training set into 80% training and 20% validation\n",
    "X_train_split, X_validation_split, y_train_split, y_validation_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the CatBoost model on the 80% of the training split\n",
    "model = CatBoostClassifier(random_state=42, verbose=0)  # Use CatBoostClassifier\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Step 4: Evaluate the model on the 20% validation split\n",
    "y_pred_validation = model.predict(X_validation_split)\n",
    "\n",
    "# Calculate validation metrics\n",
    "accuracy_validation = accuracy_score(y_validation_split, y_pred_validation)\n",
    "precision_validation = precision_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "recall_validation = recall_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "f1_validation = f1_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model on the 20% unseen testing data\n",
    "y_pred_unseen = model.predict(X_unseen_test)\n",
    "\n",
    "# Calculate unseen test metrics\n",
    "accuracy_unseen = accuracy_score(y_unseen_test, y_pred_unseen)\n",
    "precision_unseen = precision_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "recall_unseen = recall_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "f1_unseen = f1_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "\n",
    "# Output unseen test results\n",
    "print(f\"Accuracy: {accuracy_unseen:.2f}\")\n",
    "print(f\"Precision: {precision_unseen:.2f}\")\n",
    "print(f\"Recall: {recall_unseen:.2f}\")\n",
    "print(f\"F1-Score: {f1_unseen:.42}\")\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_unseen_test, y_pred_unseen)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ca10fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.21\n",
      "Precision: 98.21\n",
      "Recall: 98.21\n",
      "F1-Score: 98.20\n",
      "Overall TP: 98.39\n",
      "Overall TN: 97.82\n",
      "Overall FP: 0.89\n",
      "Overall FN: 0.89\n"
     ]
    }
   ],
   "source": [
    "#CatBoost-B\n",
    "#!pip install catboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from catboost import CatBoostClassifier  # Import CatBoostClassifier from CatBoost\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-Update-3-All_together-Bi.csv',\n",
    "    '2-Update-3-All_together-Bi.csv',\n",
    "    '3-Update-3-All_together-Bi.csv'  \n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "df['Protocol'] = label_encoder.fit_transform(df['Protocol'])\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Step 1: Split the dataset into 80% training and 20% unseen testing\n",
    "X_train, X_unseen_test, y_train, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Further split the 80% training set into 80% training and 20% validation\n",
    "X_train_split, X_validation_split, y_train_split, y_validation_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the CatBoost model on the 80% of the training split\n",
    "model = CatBoostClassifier(random_state=42, verbose=0)  # Use CatBoostClassifier\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Step 4: Evaluate the model on the 20% validation split\n",
    "y_pred_validation = model.predict(X_validation_split)\n",
    "\n",
    "# Calculate validation metrics\n",
    "accuracy_validation = accuracy_score(y_validation_split, y_pred_validation)\n",
    "precision_validation = precision_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "recall_validation = recall_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "f1_validation = f1_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model on the 20% unseen testing data\n",
    "y_pred_unseen = model.predict(X_unseen_test)\n",
    "\n",
    "# Calculate unseen test metrics\n",
    "accuracy_unseen = accuracy_score(y_unseen_test, y_pred_unseen)\n",
    "precision_unseen = precision_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "recall_unseen = recall_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "f1_unseen = f1_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "\n",
    "# Output unseen test results\n",
    "print(f\"Accuracy: {accuracy_unseen:.2f}\")\n",
    "print(f\"Precision: {precision_unseen:.2f}\")\n",
    "print(f\"Recall: {recall_unseen:.2f}\")\n",
    "print(f\"F1-Score: {f1_unseen:.42}\")\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_unseen_test, y_pred_unseen)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c71a69fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.33\n",
      "Precision: 93.36\n",
      "Recall: 93.33\n",
      "F1-Score: 93.33\n",
      "Overall TP: 93.42\n",
      "Overall TN: 92.88\n",
      "Overall FP: 6.57\n",
      "Overall FN: 6.57\n"
     ]
    }
   ],
   "source": [
    "#RF-A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import Random Forest\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-All-Together-Update-Single-26-8-24.csv',\n",
    "    '2-All-Together-Update-Single-26-8-24.csv',\n",
    "    '3-All-Together-Update-Single-26-8-24.csv',\n",
    "    '4-All-Together-Update-Single-26-8-24.csv',\n",
    "    '5-All-Together-Update-Single-26-8-24.csv'\n",
    "    #'1-Update-3-All_together-Bi.csv',\n",
    "    #'2-Update-3-All_together-Bi.csv',\n",
    "    #'3-Update-3-All_together-Bi.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "df['Protocol'] = label_encoder.fit_transform(df['Protocol'])\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Step 1: Split the dataset into 80% training and 20% unseen testing\n",
    "X_train, X_unseen_test, y_train, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Further split the 80% training set into 80% training and 20% validation\n",
    "X_train_split, X_validation_split, y_train_split, y_validation_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the Random Forest model on the 80% of the training split\n",
    "model = RandomForestClassifier(random_state=42)  # Replace with Random Forest\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Step 4: Evaluate the model on the 20% validation split\n",
    "y_pred_validation = model.predict(X_validation_split)\n",
    "\n",
    "# Calculate validation metrics\n",
    "accuracy_validation = accuracy_score(y_validation_split, y_pred_validation)\n",
    "precision_validation = precision_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "recall_validation = recall_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "f1_validation = f1_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model on the 20% unseen testing data\n",
    "y_pred_unseen = model.predict(X_unseen_test)\n",
    "\n",
    "# Calculate unseen test metrics\n",
    "accuracy_unseen = accuracy_score(y_unseen_test, y_pred_unseen)\n",
    "precision_unseen = precision_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "recall_unseen = recall_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "f1_unseen = f1_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "\n",
    "# Output unseen test results\n",
    "print(f\"Accuracy: {accuracy_unseen:.2f}\")\n",
    "print(f\"Precision: {precision_unseen:.2f}\")\n",
    "print(f\"Recall: {recall_unseen:.2f}\")\n",
    "print(f\"F1-Score: {f1_unseen:.2f}\")\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_unseen_test, y_pred_unseen)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e175f650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.82\n",
      "Precision: 87.82\n",
      "Recall: 87.82\n",
      "F1-Score: 87.78\n",
      "Overall TP: 89.08\n",
      "Overall TN: 87.91\n",
      "Overall FP: 5.55\n",
      "Overall FN: 5.55\n"
     ]
    }
   ],
   "source": [
    "#RF-B\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier  # Import Random Forest\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    \n",
    "    '1-Update-3-All_together-Bi.csv',\n",
    "    '2-Update-3-All_together-Bi.csv',\n",
    "    '3-Update-3-All_together-Bi.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "df['Protocol'] = label_encoder.fit_transform(df['Protocol'])\n",
    "df['Label'] = label_encoder.fit_transform(df['Label'])\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop(columns=['Label'])\n",
    "y = df['Label']\n",
    "\n",
    "# Step 1: Split the dataset into 80% training and 20% unseen testing\n",
    "X_train, X_unseen_test, y_train, y_unseen_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Further split the 80% training set into 80% training and 20% validation\n",
    "X_train_split, X_validation_split, y_train_split, y_validation_split = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 3: Train the Random Forest model on the 80% of the training split\n",
    "model = RandomForestClassifier(random_state=42)  # Replace with Random Forest\n",
    "model.fit(X_train_split, y_train_split)\n",
    "\n",
    "# Step 4: Evaluate the model on the 20% validation split\n",
    "y_pred_validation = model.predict(X_validation_split)\n",
    "\n",
    "# Calculate validation metrics\n",
    "accuracy_validation = accuracy_score(y_validation_split, y_pred_validation)\n",
    "precision_validation = precision_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "recall_validation = recall_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "f1_validation = f1_score(y_validation_split, y_pred_validation, average='weighted')\n",
    "\n",
    "\n",
    "# Step 5: Evaluate the model on the 20% unseen testing data\n",
    "y_pred_unseen = model.predict(X_unseen_test)\n",
    "\n",
    "# Calculate unseen test metrics\n",
    "accuracy_unseen = accuracy_score(y_unseen_test, y_pred_unseen)\n",
    "precision_unseen = precision_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "recall_unseen = recall_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "f1_unseen = f1_score(y_unseen_test, y_pred_unseen, average='weighted')\n",
    "\n",
    "# Output unseen test results\n",
    "print(f\"Accuracy: {accuracy_unseen:.2f}\")\n",
    "print(f\"Precision: {precision_unseen:.2f}\")\n",
    "print(f\"Recall: {recall_unseen:.2f}\")\n",
    "print(f\"F1-Score: {f1_unseen:.2f}\")\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_unseen_test, y_pred_unseen)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66db17c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 945us/step - accuracy: 0.3285 - loss: 1.9579 - val_accuracy: 0.4997 - val_loss: 1.4701\n",
      "Epoch 2/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 939us/step - accuracy: 0.5212 - loss: 1.4066 - val_accuracy: 0.5870 - val_loss: 1.2128\n",
      "Epoch 3/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 949us/step - accuracy: 0.6010 - loss: 1.1726 - val_accuracy: 0.6376 - val_loss: 1.0738\n",
      "Epoch 4/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 958us/step - accuracy: 0.6481 - loss: 1.0367 - val_accuracy: 0.6757 - val_loss: 0.9585\n",
      "Epoch 5/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 957us/step - accuracy: 0.6792 - loss: 0.9515 - val_accuracy: 0.6796 - val_loss: 0.9381\n",
      "Epoch 6/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 959us/step - accuracy: 0.6997 - loss: 0.8931 - val_accuracy: 0.7054 - val_loss: 0.8718\n",
      "Epoch 7/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 956us/step - accuracy: 0.7163 - loss: 0.8454 - val_accuracy: 0.7235 - val_loss: 0.8206\n",
      "Epoch 8/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 953us/step - accuracy: 0.7287 - loss: 0.8092 - val_accuracy: 0.7426 - val_loss: 0.7663\n",
      "Epoch 9/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 960us/step - accuracy: 0.7396 - loss: 0.7803 - val_accuracy: 0.7297 - val_loss: 0.8089\n",
      "Epoch 10/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 964us/step - accuracy: 0.7495 - loss: 0.7514 - val_accuracy: 0.7476 - val_loss: 0.7498\n",
      "Epoch 11/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 960us/step - accuracy: 0.7571 - loss: 0.7286 - val_accuracy: 0.7707 - val_loss: 0.7044\n",
      "Epoch 12/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 959us/step - accuracy: 0.7649 - loss: 0.7067 - val_accuracy: 0.7731 - val_loss: 0.6932\n",
      "Epoch 13/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 948us/step - accuracy: 0.7715 - loss: 0.6867 - val_accuracy: 0.7662 - val_loss: 0.7121\n",
      "Epoch 14/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 947us/step - accuracy: 0.7768 - loss: 0.6718 - val_accuracy: 0.7829 - val_loss: 0.6537\n",
      "Epoch 15/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 955us/step - accuracy: 0.7816 - loss: 0.6561 - val_accuracy: 0.7922 - val_loss: 0.6349\n",
      "Epoch 16/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 952us/step - accuracy: 0.7862 - loss: 0.6449 - val_accuracy: 0.7942 - val_loss: 0.6198\n",
      "Epoch 17/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 955us/step - accuracy: 0.7911 - loss: 0.6310 - val_accuracy: 0.7876 - val_loss: 0.6475\n",
      "Epoch 18/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 955us/step - accuracy: 0.7949 - loss: 0.6198 - val_accuracy: 0.7993 - val_loss: 0.6156\n",
      "Epoch 19/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 954us/step - accuracy: 0.7991 - loss: 0.6073 - val_accuracy: 0.8087 - val_loss: 0.5754\n",
      "Epoch 20/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 953us/step - accuracy: 0.8017 - loss: 0.5990 - val_accuracy: 0.7962 - val_loss: 0.6093\n",
      "Epoch 21/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 954us/step - accuracy: 0.8059 - loss: 0.5887 - val_accuracy: 0.8117 - val_loss: 0.5733\n",
      "Epoch 22/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 953us/step - accuracy: 0.8086 - loss: 0.5804 - val_accuracy: 0.8049 - val_loss: 0.5975\n",
      "Epoch 23/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 956us/step - accuracy: 0.8117 - loss: 0.5715 - val_accuracy: 0.8278 - val_loss: 0.5290\n",
      "Epoch 24/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 972us/step - accuracy: 0.8140 - loss: 0.5652 - val_accuracy: 0.8150 - val_loss: 0.5587\n",
      "Epoch 25/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 976us/step - accuracy: 0.8162 - loss: 0.5602 - val_accuracy: 0.8123 - val_loss: 0.5789\n",
      "Epoch 26/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 976us/step - accuracy: 0.8183 - loss: 0.5536 - val_accuracy: 0.8150 - val_loss: 0.5474\n",
      "Epoch 27/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 974us/step - accuracy: 0.8203 - loss: 0.5490 - val_accuracy: 0.8280 - val_loss: 0.5285\n",
      "Epoch 28/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 979us/step - accuracy: 0.8218 - loss: 0.5433 - val_accuracy: 0.8203 - val_loss: 0.5459\n",
      "Epoch 29/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 979us/step - accuracy: 0.8240 - loss: 0.5388 - val_accuracy: 0.8218 - val_loss: 0.5441\n",
      "Epoch 30/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 980us/step - accuracy: 0.8251 - loss: 0.5343 - val_accuracy: 0.8188 - val_loss: 0.5525\n",
      "Epoch 31/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 42s 986us/step - accuracy: 0.8269 - loss: 0.5294 - val_accuracy: 0.8379 - val_loss: 0.5037\n",
      "Epoch 32/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 981us/step - accuracy: 0.8288 - loss: 0.5237 - val_accuracy: 0.8345 - val_loss: 0.5077\n",
      "Epoch 33/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 978us/step - accuracy: 0.8307 - loss: 0.5190 - val_accuracy: 0.8315 - val_loss: 0.5233\n",
      "Epoch 34/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 975us/step - accuracy: 0.8322 - loss: 0.5142 - val_accuracy: 0.8300 - val_loss: 0.5272\n",
      "Epoch 35/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 974us/step - accuracy: 0.8333 - loss: 0.5107 - val_accuracy: 0.8362 - val_loss: 0.5139\n",
      "Epoch 36/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 973us/step - accuracy: 0.8352 - loss: 0.5075 - val_accuracy: 0.8233 - val_loss: 0.5399\n",
      "Epoch 37/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 978us/step - accuracy: 0.8364 - loss: 0.5028 - val_accuracy: 0.8164 - val_loss: 0.5606\n",
      "Epoch 38/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 974us/step - accuracy: 0.8370 - loss: 0.5013 - val_accuracy: 0.8331 - val_loss: 0.5213\n",
      "Epoch 39/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 982us/step - accuracy: 0.8383 - loss: 0.4979 - val_accuracy: 0.8482 - val_loss: 0.4743\n",
      "Epoch 40/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 978us/step - accuracy: 0.8400 - loss: 0.4917 - val_accuracy: 0.8525 - val_loss: 0.4562\n",
      "21089/21089 ━━━━━━━━━━━━━━━━━━━━ 10s 490us/step\n",
      "Accuracy: 85.23\n",
      "Precision: 85.61\n",
      "Recall: 85.24\n",
      "F1-Score: 85.29\n",
      "Overall TP: 85.29\n",
      "Overall TN: 84.75\n",
      "Overall FP: 7.23\n",
      "Overall FN: 7.23\n"
     ]
    }
   ],
   "source": [
    "#CNN-A\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models  # Use from tensorflow.keras\n",
    "\n",
    "from keras import layers\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-All-Together-Update-Single-26-8-24.csv',\n",
    "    '2-All-Together-Update-Single-26-8-24.csv',\n",
    "    '3-All-Together-Update-Single-26-8-24.csv',\n",
    "    '4-All-Together-Update-Single-26-8-24.csv',\n",
    "    '5-All-Together-Update-Single-26-8-24.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder_protocol = LabelEncoder()\n",
    "data['Protocol'] = label_encoder_protocol.fit_transform(data['Protocol'])\n",
    "\n",
    "label_encoder_label = LabelEncoder()\n",
    "data['Label'] = label_encoder_label.fit_transform(data['Label'])\n",
    "\n",
    "# Store the labels and drop unnecessary columns\n",
    "labels = data['Label'].values\n",
    "data.drop(columns=['Source IP', 'Destination IP', 'Label'], inplace=True)\n",
    "\n",
    "# Split the data into 80-20 training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into 80-20 training-validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training, validation, and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data for Conv1D layers\n",
    "X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_val_scaled = X_val_scaled.reshape((X_val_scaled.shape[0], X_val_scaled.shape[1], 1))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Define the CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train_scaled.shape[1], 1)),\n",
    "    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(np.unique(labels)), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation data\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val_scaled, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_predictions = model.predict(X_val_scaled)\n",
    "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the validation set\n",
    "val_accuracy = accuracy_score(y_val, val_predicted_labels)\n",
    "val_precision = precision_score(y_val, val_predicted_labels, average='weighted', zero_division=0)\n",
    "val_recall = recall_score(y_val, val_predicted_labels, average='weighted', zero_division=0)\n",
    "val_f1_score = f1_score(y_val, val_predicted_labels, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.4f}%')\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the test set\n",
    "test_accuracy = accuracy_score(y_test, test_predicted_labels)\n",
    "test_precision = precision_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "test_f1_score = f1_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "\n",
    "print(f'Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predicted_labels)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "102409f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 959us/step - accuracy: 0.3023 - loss: 2.0403 - val_accuracy: 0.4829 - val_loss: 1.5355\n",
      "Epoch 2/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 948us/step - accuracy: 0.5018 - loss: 1.4638 - val_accuracy: 0.5793 - val_loss: 1.2527\n",
      "Epoch 3/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 951us/step - accuracy: 0.5966 - loss: 1.1916 - val_accuracy: 0.6430 - val_loss: 1.0462\n",
      "Epoch 4/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 950us/step - accuracy: 0.6545 - loss: 1.0209 - val_accuracy: 0.6949 - val_loss: 0.9146\n",
      "Epoch 5/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 947us/step - accuracy: 0.6914 - loss: 0.9116 - val_accuracy: 0.7069 - val_loss: 0.8621\n",
      "Epoch 6/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 953us/step - accuracy: 0.7149 - loss: 0.8404 - val_accuracy: 0.7158 - val_loss: 0.8368\n",
      "Epoch 7/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 955us/step - accuracy: 0.7333 - loss: 0.7861 - val_accuracy: 0.7494 - val_loss: 0.7419\n",
      "Epoch 8/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 959us/step - accuracy: 0.7486 - loss: 0.7407 - val_accuracy: 0.7572 - val_loss: 0.7118\n",
      "Epoch 9/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 955us/step - accuracy: 0.7621 - loss: 0.7029 - val_accuracy: 0.7811 - val_loss: 0.6675\n",
      "Epoch 10/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 954us/step - accuracy: 0.7725 - loss: 0.6698 - val_accuracy: 0.7800 - val_loss: 0.6563\n",
      "Epoch 11/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 960us/step - accuracy: 0.7827 - loss: 0.6431 - val_accuracy: 0.7843 - val_loss: 0.6354\n",
      "Epoch 12/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 961us/step - accuracy: 0.7905 - loss: 0.6209 - val_accuracy: 0.8016 - val_loss: 0.5876\n",
      "Epoch 13/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 964us/step - accuracy: 0.7976 - loss: 0.6014 - val_accuracy: 0.7966 - val_loss: 0.5961\n",
      "Epoch 14/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 964us/step - accuracy: 0.8031 - loss: 0.5851 - val_accuracy: 0.8064 - val_loss: 0.5691\n",
      "Epoch 15/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 966us/step - accuracy: 0.8083 - loss: 0.5688 - val_accuracy: 0.8021 - val_loss: 0.5694\n",
      "Epoch 16/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 961us/step - accuracy: 0.8136 - loss: 0.5543 - val_accuracy: 0.8156 - val_loss: 0.5383\n",
      "Epoch 17/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 963us/step - accuracy: 0.8179 - loss: 0.5425 - val_accuracy: 0.8293 - val_loss: 0.5257\n",
      "Epoch 18/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 967us/step - accuracy: 0.8225 - loss: 0.5287 - val_accuracy: 0.8377 - val_loss: 0.4909\n",
      "Epoch 19/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 970us/step - accuracy: 0.8249 - loss: 0.5193 - val_accuracy: 0.8130 - val_loss: 0.5499\n",
      "Epoch 20/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 970us/step - accuracy: 0.8286 - loss: 0.5118 - val_accuracy: 0.8437 - val_loss: 0.4846\n",
      "Epoch 21/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 971us/step - accuracy: 0.8307 - loss: 0.5042 - val_accuracy: 0.8401 - val_loss: 0.4919\n",
      "Epoch 22/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 975us/step - accuracy: 0.8348 - loss: 0.4940 - val_accuracy: 0.8281 - val_loss: 0.5229\n",
      "Epoch 23/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 970us/step - accuracy: 0.8376 - loss: 0.4854 - val_accuracy: 0.8549 - val_loss: 0.4426\n",
      "Epoch 24/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 973us/step - accuracy: 0.8401 - loss: 0.4778 - val_accuracy: 0.8365 - val_loss: 0.4867\n",
      "Epoch 25/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 984us/step - accuracy: 0.8420 - loss: 0.4724 - val_accuracy: 0.8508 - val_loss: 0.4593\n",
      "Epoch 26/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 984us/step - accuracy: 0.8446 - loss: 0.4647 - val_accuracy: 0.8491 - val_loss: 0.4641\n",
      "Epoch 27/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 976us/step - accuracy: 0.8467 - loss: 0.4575 - val_accuracy: 0.8443 - val_loss: 0.4895\n",
      "Epoch 28/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 977us/step - accuracy: 0.8491 - loss: 0.4519 - val_accuracy: 0.8585 - val_loss: 0.4331\n",
      "Epoch 29/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 981us/step - accuracy: 0.8512 - loss: 0.4458 - val_accuracy: 0.8565 - val_loss: 0.4304\n",
      "Epoch 30/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 981us/step - accuracy: 0.8517 - loss: 0.4431 - val_accuracy: 0.8518 - val_loss: 0.4440\n",
      "Epoch 31/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 982us/step - accuracy: 0.8552 - loss: 0.4362 - val_accuracy: 0.8544 - val_loss: 0.4474\n",
      "Epoch 32/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 980us/step - accuracy: 0.8564 - loss: 0.4313 - val_accuracy: 0.8440 - val_loss: 0.4850\n",
      "Epoch 33/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 976us/step - accuracy: 0.8577 - loss: 0.4259 - val_accuracy: 0.8696 - val_loss: 0.3964\n",
      "Epoch 34/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 967us/step - accuracy: 0.8602 - loss: 0.4187 - val_accuracy: 0.8619 - val_loss: 0.4132\n",
      "Epoch 35/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 21s 1ms/step - accuracy: 0.8626 - loss: 0.4119 - val_accuracy: 0.8553 - val_loss: 0.4308\n",
      "Epoch 36/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 28s 1ms/step - accuracy: 0.8635 - loss: 0.4107 - val_accuracy: 0.8222 - val_loss: 0.5308\n",
      "Epoch 37/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 28s 1ms/step - accuracy: 0.8646 - loss: 0.4065 - val_accuracy: 0.8638 - val_loss: 0.4006\n",
      "Epoch 38/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 28s 1ms/step - accuracy: 0.8669 - loss: 0.4006 - val_accuracy: 0.8621 - val_loss: 0.4210\n",
      "Epoch 39/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 24s 1ms/step - accuracy: 0.8684 - loss: 0.3971 - val_accuracy: 0.8764 - val_loss: 0.3803\n",
      "Epoch 40/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 976us/step - accuracy: 0.8705 - loss: 0.3907 - val_accuracy: 0.8774 - val_loss: 0.3744\n",
      "10206/10206 ━━━━━━━━━━━━━━━━━━━━ 5s 504us/step\n",
      "\n",
      "Accuracy: 87.72\n",
      "Precision: 87.89\n",
      "Recall: 87.73\n",
      "F1-Score: 87.67\n",
      "Overall TP: 87.65\n",
      "Overall TN: 86.96\n",
      "Overall FP: 5.57\n",
      "Overall FN: 5.57\n"
     ]
    }
   ],
   "source": [
    "#CNN-B\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models  # Use from tensorflow.keras\n",
    "\n",
    "from keras import layers\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "     '1-Update-3-All_together-Bi.csv',\n",
    "    '2-Update-3-All_together-Bi.csv',\n",
    "    '3-Update-3-All_together-Bi.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder_protocol = LabelEncoder()\n",
    "data['Protocol'] = label_encoder_protocol.fit_transform(data['Protocol'])\n",
    "\n",
    "label_encoder_label = LabelEncoder()\n",
    "data['Label'] = label_encoder_label.fit_transform(data['Label'])\n",
    "\n",
    "# Store the labels and drop unnecessary columns\n",
    "labels = data['Label'].values\n",
    "data.drop(columns=['Source IP', 'Destination IP', 'Label'], inplace=True)\n",
    "\n",
    "# Split the data into 80-20 training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into 80-20 training-validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training, validation, and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data for Conv1D layers\n",
    "X_train_scaled = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_val_scaled = X_val_scaled.reshape((X_val_scaled.shape[0], X_val_scaled.shape[1], 1))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Define the CNN model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train_scaled.shape[1], 1)),\n",
    "    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(np.unique(labels)), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation data\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val_scaled, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "val_predictions = model.predict(X_val_scaled)\n",
    "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the validation set\n",
    "val_accuracy = accuracy_score(y_val, val_predicted_labels)\n",
    "val_precision = precision_score(y_val, val_predicted_labels, average='weighted', zero_division=0)\n",
    "val_recall = recall_score(y_val, val_predicted_labels, average='weighted', zero_division=0)\n",
    "val_f1_score = f1_score(y_val, val_predicted_labels, average='weighted', zero_division=0)\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.4f}%')\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = model.predict(X_test_scaled)\n",
    "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the test set\n",
    "test_accuracy = accuracy_score(y_test, test_predicted_labels)\n",
    "test_precision = precision_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "test_f1_score = f1_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "\n",
    "print(f'Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predicted_labels)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68eee128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 46s 1ms/step - accuracy: 0.3973 - loss: 1.7740 - val_accuracy: 0.6370 - val_loss: 1.1144\n",
      "Epoch 2/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.6588 - loss: 1.0388 - val_accuracy: 0.7163 - val_loss: 0.8656\n",
      "Epoch 3/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 44s 1ms/step - accuracy: 0.7297 - loss: 0.8373 - val_accuracy: 0.7609 - val_loss: 0.7501\n",
      "Epoch 4/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.7685 - loss: 0.7257 - val_accuracy: 0.7892 - val_loss: 0.6670\n",
      "Epoch 5/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 44s 1ms/step - accuracy: 0.7927 - loss: 0.6556 - val_accuracy: 0.8091 - val_loss: 0.6069\n",
      "Epoch 6/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8120 - loss: 0.6031 - val_accuracy: 0.8203 - val_loss: 0.5719\n",
      "Epoch 7/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8257 - loss: 0.5623 - val_accuracy: 0.8325 - val_loss: 0.5408\n",
      "Epoch 8/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 44s 1ms/step - accuracy: 0.8354 - loss: 0.5324 - val_accuracy: 0.8461 - val_loss: 0.5116\n",
      "Epoch 9/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8442 - loss: 0.5077 - val_accuracy: 0.8409 - val_loss: 0.5176\n",
      "Epoch 10/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8517 - loss: 0.4862 - val_accuracy: 0.8618 - val_loss: 0.4633\n",
      "Epoch 11/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8592 - loss: 0.4664 - val_accuracy: 0.8666 - val_loss: 0.4451\n",
      "Epoch 12/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8654 - loss: 0.4486 - val_accuracy: 0.8723 - val_loss: 0.4265\n",
      "Epoch 13/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8708 - loss: 0.4348 - val_accuracy: 0.8757 - val_loss: 0.4137\n",
      "Epoch 14/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8755 - loss: 0.4201 - val_accuracy: 0.8785 - val_loss: 0.4014\n",
      "Epoch 15/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8791 - loss: 0.4084 - val_accuracy: 0.8838 - val_loss: 0.3994\n",
      "Epoch 16/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8834 - loss: 0.3967 - val_accuracy: 0.8819 - val_loss: 0.3972\n",
      "Epoch 17/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8866 - loss: 0.3870 - val_accuracy: 0.8901 - val_loss: 0.3812\n",
      "Epoch 18/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8902 - loss: 0.3762 - val_accuracy: 0.8925 - val_loss: 0.3716\n",
      "Epoch 19/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8934 - loss: 0.3653 - val_accuracy: 0.8927 - val_loss: 0.3671\n",
      "Epoch 20/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8961 - loss: 0.3574 - val_accuracy: 0.8929 - val_loss: 0.3635\n",
      "Epoch 21/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.8976 - loss: 0.3527 - val_accuracy: 0.8995 - val_loss: 0.3442\n",
      "Epoch 22/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9002 - loss: 0.3448 - val_accuracy: 0.9011 - val_loss: 0.3388\n",
      "Epoch 23/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9023 - loss: 0.3380 - val_accuracy: 0.9023 - val_loss: 0.3369\n",
      "Epoch 24/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9037 - loss: 0.3331 - val_accuracy: 0.9055 - val_loss: 0.3260\n",
      "Epoch 25/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9061 - loss: 0.3254 - val_accuracy: 0.9049 - val_loss: 0.3388\n",
      "Epoch 26/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9074 - loss: 0.3224 - val_accuracy: 0.9083 - val_loss: 0.3258\n",
      "Epoch 27/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9089 - loss: 0.3160 - val_accuracy: 0.9113 - val_loss: 0.3107\n",
      "Epoch 28/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9114 - loss: 0.3094 - val_accuracy: 0.9091 - val_loss: 0.3140\n",
      "Epoch 29/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9127 - loss: 0.3060 - val_accuracy: 0.9045 - val_loss: 0.3337\n",
      "Epoch 30/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9137 - loss: 0.3015 - val_accuracy: 0.9150 - val_loss: 0.2907\n",
      "Epoch 31/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9149 - loss: 0.2969 - val_accuracy: 0.9171 - val_loss: 0.2962\n",
      "Epoch 32/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9168 - loss: 0.2926 - val_accuracy: 0.9176 - val_loss: 0.2909\n",
      "Epoch 33/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9174 - loss: 0.2931 - val_accuracy: 0.9181 - val_loss: 0.2910\n",
      "Epoch 34/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9187 - loss: 0.2868 - val_accuracy: 0.9165 - val_loss: 0.2881\n",
      "Epoch 35/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 46s 1ms/step - accuracy: 0.9203 - loss: 0.2824 - val_accuracy: 0.9133 - val_loss: 0.3020\n",
      "Epoch 36/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 46s 1ms/step - accuracy: 0.9207 - loss: 0.2794 - val_accuracy: 0.9219 - val_loss: 0.2783\n",
      "Epoch 37/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9214 - loss: 0.2792 - val_accuracy: 0.9219 - val_loss: 0.2800\n",
      "Epoch 38/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9230 - loss: 0.2725 - val_accuracy: 0.9239 - val_loss: 0.2715\n",
      "Epoch 39/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9244 - loss: 0.2691 - val_accuracy: 0.9261 - val_loss: 0.2680\n",
      "Epoch 40/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 45s 1ms/step - accuracy: 0.9245 - loss: 0.2679 - val_accuracy: 0.9246 - val_loss: 0.2659\n",
      "21089/21089 ━━━━━━━━━━━━━━━━━━━━ 11s 496us/step\n",
      "Accuracy: 92.35\n",
      "Precision: 92.41\n",
      "Recall: 92.35\n",
      "F1-Score: 92.32\n",
      "Overall TP: 92.39\n",
      "Overall TN: 91.68\n",
      "Overall FP: 4.12\n",
      "Overall FN: 4.12\n"
     ]
    }
   ],
   "source": [
    "#LSTM-A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-All-Together-Update-Single-26-8-24.csv',\n",
    "    '2-All-Together-Update-Single-26-8-24.csv',\n",
    "    '3-All-Together-Update-Single-26-8-24.csv',\n",
    "    '4-All-Together-Update-Single-26-8-24.csv',\n",
    "    '5-All-Together-Update-Single-26-8-24.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['Source IP', 'Destination IP', 'Label'])\n",
    "y = data['Label']\n",
    "\n",
    "# Step 1: Split into Train, Validation, and Test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Initialize the scaler and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Reshape the data for LSTM input (LSTM expects 3D input: [samples, timesteps, features])\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# Step 4: Define the LSTM model using Functional API\n",
    "inputs = layers.Input(shape=(1, X_train_reshaped.shape[2]))\n",
    "x = layers.LSTM(128, activation='tanh', return_sequences=True)(inputs)\n",
    "x = layers.LSTM(64, activation='tanh')(x)\n",
    "outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)\n",
    "lstm_model = Model(inputs, outputs)\n",
    "\n",
    "# Compile and train the LSTM model\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train_reshaped, y_train, epochs=40, batch_size=64, validation_data=(X_val_reshaped, y_val))\n",
    "\n",
    "# --- Validation Metrics ---\n",
    "# Make predictions with the LSTM model on validation set\n",
    "y_pred_val = np.argmax(lstm_model.predict(X_val_reshaped), axis=-1)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_precision = precision_score(y_val, y_pred_val, average='weighted')\n",
    "val_recall = recall_score(y_val, y_pred_val, average='weighted')\n",
    "val_f1_score = f1_score(y_val, y_pred_val, average='weighted')\n",
    "\n",
    "# --- Test Metrics ---\n",
    "# Make predictions with the LSTM model on the test set\n",
    "y_pred_test = np.argmax(lstm_model.predict(X_test_reshaped), axis=-1)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_score = f1_score(y_test, y_pred_test, average='weighted')\n",
    "\n",
    "print(f'\\nAccuracy:{test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1a475b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 23s 1ms/step - accuracy: 0.3281 - loss: 1.9645 - val_accuracy: 0.5234 - val_loss: 1.4337\n",
      "Epoch 2/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.5584 - loss: 1.3279 - val_accuracy: 0.6272 - val_loss: 1.1195\n",
      "Epoch 3/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 21s 1ms/step - accuracy: 0.6442 - loss: 1.0774 - val_accuracy: 0.6809 - val_loss: 0.9649\n",
      "Epoch 4/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.6915 - loss: 0.9435 - val_accuracy: 0.7135 - val_loss: 0.8741\n",
      "Epoch 5/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.7219 - loss: 0.8560 - val_accuracy: 0.7403 - val_loss: 0.8070\n",
      "Epoch 6/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 21s 1ms/step - accuracy: 0.7456 - loss: 0.7899 - val_accuracy: 0.7620 - val_loss: 0.7457\n",
      "Epoch 7/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.7647 - loss: 0.7358 - val_accuracy: 0.7751 - val_loss: 0.7071\n",
      "Epoch 8/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.7806 - loss: 0.6931 - val_accuracy: 0.7887 - val_loss: 0.6752\n",
      "Epoch 9/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.7910 - loss: 0.6619 - val_accuracy: 0.8014 - val_loss: 0.6333\n",
      "Epoch 10/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8009 - loss: 0.6309 - val_accuracy: 0.8036 - val_loss: 0.6146\n",
      "Epoch 11/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8088 - loss: 0.6071 - val_accuracy: 0.8117 - val_loss: 0.5954\n",
      "Epoch 12/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8161 - loss: 0.5841 - val_accuracy: 0.8237 - val_loss: 0.5623\n",
      "Epoch 13/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8225 - loss: 0.5645 - val_accuracy: 0.8224 - val_loss: 0.5515\n",
      "Epoch 14/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8288 - loss: 0.5439 - val_accuracy: 0.8288 - val_loss: 0.5641\n",
      "Epoch 15/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8345 - loss: 0.5284 - val_accuracy: 0.8404 - val_loss: 0.5127\n",
      "Epoch 16/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8387 - loss: 0.5172 - val_accuracy: 0.8383 - val_loss: 0.5186\n",
      "Epoch 17/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8432 - loss: 0.5024 - val_accuracy: 0.8444 - val_loss: 0.4884\n",
      "Epoch 18/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8484 - loss: 0.4862 - val_accuracy: 0.8470 - val_loss: 0.4807\n",
      "Epoch 19/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8517 - loss: 0.4757 - val_accuracy: 0.8503 - val_loss: 0.4778\n",
      "Epoch 20/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8555 - loss: 0.4624 - val_accuracy: 0.8530 - val_loss: 0.4701\n",
      "Epoch 21/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8580 - loss: 0.4546 - val_accuracy: 0.8544 - val_loss: 0.4626\n",
      "Epoch 22/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8615 - loss: 0.4417 - val_accuracy: 0.8646 - val_loss: 0.4353\n",
      "Epoch 23/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8660 - loss: 0.4289 - val_accuracy: 0.8672 - val_loss: 0.4313\n",
      "Epoch 24/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8688 - loss: 0.4207 - val_accuracy: 0.8746 - val_loss: 0.4020\n",
      "Epoch 25/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8710 - loss: 0.4142 - val_accuracy: 0.8652 - val_loss: 0.4363\n",
      "Epoch 26/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8728 - loss: 0.4084 - val_accuracy: 0.8731 - val_loss: 0.4087\n",
      "Epoch 27/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8749 - loss: 0.4030 - val_accuracy: 0.8768 - val_loss: 0.4007\n",
      "Epoch 28/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8774 - loss: 0.3949 - val_accuracy: 0.8748 - val_loss: 0.4132\n",
      "Epoch 29/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8799 - loss: 0.3884 - val_accuracy: 0.8811 - val_loss: 0.3845\n",
      "Epoch 30/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8826 - loss: 0.3791 - val_accuracy: 0.8802 - val_loss: 0.3924\n",
      "Epoch 31/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8838 - loss: 0.3775 - val_accuracy: 0.8879 - val_loss: 0.3685\n",
      "Epoch 32/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8862 - loss: 0.3701 - val_accuracy: 0.8946 - val_loss: 0.3512\n",
      "Epoch 33/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8876 - loss: 0.3665 - val_accuracy: 0.8887 - val_loss: 0.3725\n",
      "Epoch 34/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8891 - loss: 0.3612 - val_accuracy: 0.8924 - val_loss: 0.3518\n",
      "Epoch 35/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8909 - loss: 0.3560 - val_accuracy: 0.8973 - val_loss: 0.3377\n",
      "Epoch 36/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8927 - loss: 0.3501 - val_accuracy: 0.8938 - val_loss: 0.3471\n",
      "Epoch 37/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8945 - loss: 0.3481 - val_accuracy: 0.8891 - val_loss: 0.3732\n",
      "Epoch 38/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8963 - loss: 0.3415 - val_accuracy: 0.8864 - val_loss: 0.3760\n",
      "Epoch 39/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8974 - loss: 0.3391 - val_accuracy: 0.8970 - val_loss: 0.3426\n",
      "Epoch 40/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step - accuracy: 0.8986 - loss: 0.3336 - val_accuracy: 0.9025 - val_loss: 0.3222\n",
      "10206/10206 ━━━━━━━━━━━━━━━━━━━━ 5s 496us/step\n",
      "\n",
      "Accuracy: 90.16\n",
      "Precision: 90.16\n",
      "Recall: 90.17\n",
      "F1-Score: 90.09\n",
      "Overall TP: 90.05\n",
      "Overall TN: 88.92\n",
      "Overall FP: 4.58\n",
      "Overall FN: 4.58\n"
     ]
    }
   ],
   "source": [
    "#LSTM-B\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-Update-3-All_together-Bi.csv',\n",
    "    '2-Update-3-All_together-Bi.csv',\n",
    "    '3-Update-3-All_together-Bi.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['Source IP', 'Destination IP', 'Label'])\n",
    "y = data['Label']\n",
    "\n",
    "# Step 1: Split into Train, Validation, and Test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Initialize the scaler and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Reshape the data for LSTM input (LSTM expects 3D input: [samples, timesteps, features])\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# Step 4: Define the LSTM model using Functional API\n",
    "inputs = layers.Input(shape=(1, X_train_reshaped.shape[2]))\n",
    "x = layers.LSTM(128, activation='tanh', return_sequences=True)(inputs)\n",
    "x = layers.LSTM(64, activation='tanh')(x)\n",
    "outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)\n",
    "lstm_model = Model(inputs, outputs)\n",
    "\n",
    "# Compile and train the LSTM model\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train_reshaped, y_train, epochs=40, batch_size=64, validation_data=(X_val_reshaped, y_val))\n",
    "\n",
    "# --- Validation Metrics ---\n",
    "# Make predictions with the LSTM model on validation set\n",
    "y_pred_val = np.argmax(lstm_model.predict(X_val_reshaped), axis=-1)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_precision = precision_score(y_val, y_pred_val, average='weighted')\n",
    "val_recall = recall_score(y_val, y_pred_val, average='weighted')\n",
    "val_f1_score = f1_score(y_val, y_pred_val, average='weighted')\n",
    "\n",
    "# --- Test Metrics ---\n",
    "# Make predictions with the LSTM model on the test set\n",
    "y_pred_test = np.argmax(lstm_model.predict(X_test_reshaped), axis=-1)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_score = f1_score(y_test, y_pred_test, average='weighted')\n",
    "\n",
    "print(f'\\nAccuracy:{test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "30692249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 456us/step - accuracy: 0.3273 - loss: 1.9555 - val_accuracy: 0.4501 - val_loss: 1.5869\n",
      "Epoch 2/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 472us/step - accuracy: 0.4807 - loss: 1.5248 - val_accuracy: 0.5258 - val_loss: 1.3970\n",
      "Epoch 3/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 475us/step - accuracy: 0.5373 - loss: 1.3556 - val_accuracy: 0.5593 - val_loss: 1.2849\n",
      "Epoch 4/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 474us/step - accuracy: 0.5719 - loss: 1.2581 - val_accuracy: 0.5971 - val_loss: 1.1893\n",
      "Epoch 5/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 477us/step - accuracy: 0.5972 - loss: 1.1847 - val_accuracy: 0.6006 - val_loss: 1.1671\n",
      "Epoch 6/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 485us/step - accuracy: 0.6144 - loss: 1.1353 - val_accuracy: 0.6229 - val_loss: 1.1097\n",
      "Epoch 7/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 474us/step - accuracy: 0.6295 - loss: 1.0940 - val_accuracy: 0.6509 - val_loss: 1.0480\n",
      "Epoch 8/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 478us/step - accuracy: 0.6420 - loss: 1.0612 - val_accuracy: 0.6402 - val_loss: 1.0529\n",
      "Epoch 9/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 479us/step - accuracy: 0.6521 - loss: 1.0336 - val_accuracy: 0.6621 - val_loss: 1.0084\n",
      "Epoch 10/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 479us/step - accuracy: 0.6610 - loss: 1.0091 - val_accuracy: 0.6617 - val_loss: 1.0161\n",
      "Epoch 11/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 475us/step - accuracy: 0.6687 - loss: 0.9891 - val_accuracy: 0.6863 - val_loss: 0.9445\n",
      "Epoch 12/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 482us/step - accuracy: 0.6763 - loss: 0.9681 - val_accuracy: 0.6743 - val_loss: 0.9475\n",
      "Epoch 13/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 479us/step - accuracy: 0.6824 - loss: 0.9499 - val_accuracy: 0.6918 - val_loss: 0.9234\n",
      "Epoch 14/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 479us/step - accuracy: 0.6889 - loss: 0.9317 - val_accuracy: 0.6999 - val_loss: 0.9089\n",
      "Epoch 15/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 474us/step - accuracy: 0.6950 - loss: 0.9172 - val_accuracy: 0.6993 - val_loss: 0.9148\n",
      "Epoch 16/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 481us/step - accuracy: 0.7005 - loss: 0.9006 - val_accuracy: 0.6929 - val_loss: 0.9254\n",
      "Epoch 17/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 477us/step - accuracy: 0.7049 - loss: 0.8869 - val_accuracy: 0.7165 - val_loss: 0.8586\n",
      "Epoch 18/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 482us/step - accuracy: 0.7097 - loss: 0.8728 - val_accuracy: 0.7187 - val_loss: 0.8467\n",
      "Epoch 19/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 486us/step - accuracy: 0.7141 - loss: 0.8605 - val_accuracy: 0.7188 - val_loss: 0.8418\n",
      "Epoch 20/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 483us/step - accuracy: 0.7173 - loss: 0.8498 - val_accuracy: 0.7217 - val_loss: 0.8350\n",
      "Epoch 21/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 484us/step - accuracy: 0.7212 - loss: 0.8392 - val_accuracy: 0.7141 - val_loss: 0.8656\n",
      "Epoch 22/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 482us/step - accuracy: 0.7247 - loss: 0.8280 - val_accuracy: 0.7156 - val_loss: 0.8626\n",
      "Epoch 23/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 487us/step - accuracy: 0.7276 - loss: 0.8187 - val_accuracy: 0.7285 - val_loss: 0.8082\n",
      "Epoch 24/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 485us/step - accuracy: 0.7312 - loss: 0.8079 - val_accuracy: 0.7215 - val_loss: 0.8253\n",
      "Epoch 25/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 485us/step - accuracy: 0.7333 - loss: 0.8023 - val_accuracy: 0.7318 - val_loss: 0.8010\n",
      "Epoch 26/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 485us/step - accuracy: 0.7352 - loss: 0.7954 - val_accuracy: 0.7319 - val_loss: 0.7944\n",
      "Epoch 27/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 486us/step - accuracy: 0.7383 - loss: 0.7856 - val_accuracy: 0.7349 - val_loss: 0.8033\n",
      "Epoch 28/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 20s 484us/step - accuracy: 0.7414 - loss: 0.7766 - val_accuracy: 0.7459 - val_loss: 0.7641\n",
      "Epoch 29/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 485us/step - accuracy: 0.7436 - loss: 0.7712 - val_accuracy: 0.7475 - val_loss: 0.7638\n",
      "Epoch 30/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 490us/step - accuracy: 0.7457 - loss: 0.7663 - val_accuracy: 0.7426 - val_loss: 0.7839\n",
      "Epoch 31/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 488us/step - accuracy: 0.7477 - loss: 0.7589 - val_accuracy: 0.7517 - val_loss: 0.7572\n",
      "Epoch 32/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 491us/step - accuracy: 0.7485 - loss: 0.7573 - val_accuracy: 0.7524 - val_loss: 0.7456\n",
      "Epoch 33/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 486us/step - accuracy: 0.7514 - loss: 0.7505 - val_accuracy: 0.7495 - val_loss: 0.7461\n",
      "Epoch 34/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 487us/step - accuracy: 0.7526 - loss: 0.7468 - val_accuracy: 0.7507 - val_loss: 0.7587\n",
      "Epoch 35/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 488us/step - accuracy: 0.7537 - loss: 0.7418 - val_accuracy: 0.7625 - val_loss: 0.7255\n",
      "Epoch 36/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 490us/step - accuracy: 0.7556 - loss: 0.7368 - val_accuracy: 0.7538 - val_loss: 0.7456\n",
      "Epoch 37/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 491us/step - accuracy: 0.7576 - loss: 0.7345 - val_accuracy: 0.7598 - val_loss: 0.7194\n",
      "Epoch 38/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 492us/step - accuracy: 0.7580 - loss: 0.7303 - val_accuracy: 0.7578 - val_loss: 0.7241\n",
      "Epoch 39/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 490us/step - accuracy: 0.7595 - loss: 0.7264 - val_accuracy: 0.7621 - val_loss: 0.7079\n",
      "Epoch 40/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 21s 493us/step - accuracy: 0.7611 - loss: 0.7223 - val_accuracy: 0.7700 - val_loss: 0.7173\n",
      "26362/26362 ━━━━━━━━━━━━━━━━━━━━ 9s 334us/step - accuracy: 0.7715 - loss: 0.7140\n",
      "Test Accuracy: 77.0900%\n",
      "26362/26362 ━━━━━━━━━━━━━━━━━━━━ 9s 352us/step\n",
      "\n",
      "Accuracy: 77.00\n",
      "Precision: 77.67\n",
      "Recall: 77.00\n",
      "F1-Score: 76.85\n",
      "Overall TP: 76.99\n",
      "Overall TN: 76.32\n",
      "Overall FP: 11.85\n",
      "Overall FN: 11.85\n"
     ]
    }
   ],
   "source": [
    "#FFNN-A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-All-Together-Update-Single-26-8-24.csv',\n",
    "    '2-All-Together-Update-Single-26-8-24.csv',\n",
    "    '3-All-Together-Update-Single-26-8-24.csv',\n",
    "    '4-All-Together-Update-Single-26-8-24.csv',\n",
    "    '5-All-Together-Update-Single-26-8-24.csv'\n",
    "    \n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['Source IP', 'Destination IP', 'Label'])\n",
    "y = data['Label']\n",
    "\n",
    "# Step 1: Split into Train, Validation, and Test sets\n",
    "# First, split into Train+Validation and Test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, split Train+Validation into Train and Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)  # 0.25 x 0.8 = 0.2 of total data\n",
    "\n",
    "# Step 2: Initialize the scaler and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Scale the validation and test sets using the scaler fitted on the training data\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the feedforward neural network model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(len(np.unique(y)), activation='softmax')  # Adjust based on the number of target classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation data\n",
    "model.fit(X_train_scaled, y_train, epochs=40, batch_size=64, validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.4f}%')\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_test = model.predict(X_test_scaled)\n",
    "predicted_labels_test = np.argmax(predictions_test, axis=1)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the test set\n",
    "test_accuracy = accuracy_score(y_test, predicted_labels_test)\n",
    "test_precision = precision_score(y_test, predicted_labels_test, average='weighted')\n",
    "test_recall = recall_score(y_test, predicted_labels_test, average='weighted')\n",
    "test_f1_score = f1_score(y_test, predicted_labels_test, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels_test)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "056ba42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 528us/step - accuracy: 0.2873 - loss: 2.0690 - val_accuracy: 0.3874 - val_loss: 1.7782\n",
      "Epoch 2/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 9s 521us/step - accuracy: 0.4084 - loss: 1.7221 - val_accuracy: 0.4560 - val_loss: 1.5946\n",
      "Epoch 3/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 541us/step - accuracy: 0.4646 - loss: 1.5658 - val_accuracy: 0.4956 - val_loss: 1.4947\n",
      "Epoch 4/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 530us/step - accuracy: 0.5028 - loss: 1.4662 - val_accuracy: 0.5215 - val_loss: 1.4068\n",
      "Epoch 5/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 9s 526us/step - accuracy: 0.5295 - loss: 1.3926 - val_accuracy: 0.5312 - val_loss: 1.3713\n",
      "Epoch 6/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 9s 527us/step - accuracy: 0.5502 - loss: 1.3345 - val_accuracy: 0.5670 - val_loss: 1.2976\n",
      "Epoch 7/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 9s 528us/step - accuracy: 0.5677 - loss: 1.2854 - val_accuracy: 0.5592 - val_loss: 1.2843\n",
      "Epoch 8/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 532us/step - accuracy: 0.5824 - loss: 1.2432 - val_accuracy: 0.5863 - val_loss: 1.2283\n",
      "Epoch 9/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 9s 519us/step - accuracy: 0.5924 - loss: 1.2117 - val_accuracy: 0.6019 - val_loss: 1.1902\n",
      "Epoch 10/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 9s 512us/step - accuracy: 0.6030 - loss: 1.1837 - val_accuracy: 0.6055 - val_loss: 1.1713\n",
      "Epoch 11/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 9s 523us/step - accuracy: 0.6105 - loss: 1.1597 - val_accuracy: 0.6198 - val_loss: 1.1428\n",
      "Epoch 12/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 538us/step - accuracy: 0.6176 - loss: 1.1401 - val_accuracy: 0.6177 - val_loss: 1.1282\n",
      "Epoch 13/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 535us/step - accuracy: 0.6244 - loss: 1.1197 - val_accuracy: 0.6239 - val_loss: 1.1141\n",
      "Epoch 14/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 538us/step - accuracy: 0.6299 - loss: 1.1028 - val_accuracy: 0.6372 - val_loss: 1.0838\n",
      "Epoch 15/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 537us/step - accuracy: 0.6358 - loss: 1.0841 - val_accuracy: 0.6440 - val_loss: 1.0706\n",
      "Epoch 16/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 534us/step - accuracy: 0.6402 - loss: 1.0711 - val_accuracy: 0.6274 - val_loss: 1.0939\n",
      "Epoch 17/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 9s 529us/step - accuracy: 0.6448 - loss: 1.0575 - val_accuracy: 0.6526 - val_loss: 1.0349\n",
      "Epoch 18/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 538us/step - accuracy: 0.6487 - loss: 1.0422 - val_accuracy: 0.6623 - val_loss: 1.0211\n",
      "Epoch 19/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 549us/step - accuracy: 0.6544 - loss: 1.0285 - val_accuracy: 0.6421 - val_loss: 1.0608\n",
      "Epoch 20/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 533us/step - accuracy: 0.6577 - loss: 1.0178 - val_accuracy: 0.6510 - val_loss: 1.0458\n",
      "Epoch 21/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 535us/step - accuracy: 0.6614 - loss: 1.0066 - val_accuracy: 0.6620 - val_loss: 1.0079\n",
      "Epoch 22/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 532us/step - accuracy: 0.6656 - loss: 0.9963 - val_accuracy: 0.6579 - val_loss: 1.0062\n",
      "Epoch 23/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 533us/step - accuracy: 0.6688 - loss: 0.9846 - val_accuracy: 0.6665 - val_loss: 0.9963\n",
      "Epoch 24/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 534us/step - accuracy: 0.6709 - loss: 0.9788 - val_accuracy: 0.6686 - val_loss: 0.9828\n",
      "Epoch 25/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 531us/step - accuracy: 0.6749 - loss: 0.9684 - val_accuracy: 0.6795 - val_loss: 0.9631\n",
      "Epoch 26/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 9s 529us/step - accuracy: 0.6775 - loss: 0.9606 - val_accuracy: 0.6832 - val_loss: 0.9532\n",
      "Epoch 27/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 9s 529us/step - accuracy: 0.6806 - loss: 0.9529 - val_accuracy: 0.6722 - val_loss: 0.9757\n",
      "Epoch 28/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 9s 529us/step - accuracy: 0.6833 - loss: 0.9456 - val_accuracy: 0.6755 - val_loss: 0.9488\n",
      "Epoch 29/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 531us/step - accuracy: 0.6874 - loss: 0.9374 - val_accuracy: 0.6803 - val_loss: 0.9550\n",
      "Epoch 30/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 531us/step - accuracy: 0.6894 - loss: 0.9312 - val_accuracy: 0.6849 - val_loss: 0.9485\n",
      "Epoch 31/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 532us/step - accuracy: 0.6905 - loss: 0.9257 - val_accuracy: 0.6814 - val_loss: 0.9450\n",
      "Epoch 32/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 530us/step - accuracy: 0.6926 - loss: 0.9215 - val_accuracy: 0.6914 - val_loss: 0.9202\n",
      "Epoch 33/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 531us/step - accuracy: 0.6950 - loss: 0.9151 - val_accuracy: 0.6960 - val_loss: 0.9132\n",
      "Epoch 34/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 537us/step - accuracy: 0.6978 - loss: 0.9078 - val_accuracy: 0.7015 - val_loss: 0.8903\n",
      "Epoch 35/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 532us/step - accuracy: 0.7008 - loss: 0.8988 - val_accuracy: 0.7028 - val_loss: 0.9011\n",
      "Epoch 36/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 534us/step - accuracy: 0.7013 - loss: 0.8988 - val_accuracy: 0.6905 - val_loss: 0.9208\n",
      "Epoch 37/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 531us/step - accuracy: 0.7041 - loss: 0.8904 - val_accuracy: 0.7014 - val_loss: 0.8964\n",
      "Epoch 38/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 533us/step - accuracy: 0.7055 - loss: 0.8837 - val_accuracy: 0.7090 - val_loss: 0.8744\n",
      "Epoch 39/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 535us/step - accuracy: 0.7069 - loss: 0.8807 - val_accuracy: 0.7039 - val_loss: 0.8951\n",
      "Epoch 40/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 10s 534us/step - accuracy: 0.7096 - loss: 0.8749 - val_accuracy: 0.7210 - val_loss: 0.8565\n",
      "12757/12757 ━━━━━━━━━━━━━━━━━━━━ 4s 328us/step - accuracy: 0.7213 - loss: 0.8571\n",
      "Test Accuracy: 72.1281%\n",
      "12757/12757 ━━━━━━━━━━━━━━━━━━━━ 4s 345us/step\n",
      "Accuracy: 72.09\n",
      "Precision: 73.13\n",
      "Recall: 72.10\n",
      "F1-Score: 72.26\n",
      "Overall TP: 72.05\n",
      "Overall TN: 71.37\n",
      "Overall FP: 2.76\n",
      "Overall FN: 2.76\n"
     ]
    }
   ],
   "source": [
    "#FFNN-B\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-Update-3-All_together-Bi.csv',\n",
    "    '2-Update-3-All_together-Bi.csv',\n",
    "    '3-Update-3-All_together-Bi.csv' \n",
    "    \n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['Source IP', 'Destination IP', 'Label'])\n",
    "y = data['Label']\n",
    "\n",
    "# Step 1: Split into Train, Validation, and Test sets\n",
    "# First, split into Train+Validation and Test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, split Train+Validation into Train and Validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)  # 0.25 x 0.8 = 0.2 of total data\n",
    "\n",
    "# Step 2: Initialize the scaler and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Scale the validation and test sets using the scaler fitted on the training data\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the feedforward neural network model\n",
    "model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(len(np.unique(y)), activation='softmax')  # Adjust based on the number of target classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model with validation data\n",
    "model.fit(X_train_scaled, y_train, epochs=40, batch_size=64, validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.4f}%')\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_test = model.predict(X_test_scaled)\n",
    "predicted_labels_test = np.argmax(predictions_test, axis=1)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the test set\n",
    "test_accuracy = accuracy_score(y_test, predicted_labels_test)\n",
    "test_precision = precision_score(y_test, predicted_labels_test, average='weighted')\n",
    "test_recall = recall_score(y_test, predicted_labels_test, average='weighted')\n",
    "test_f1_score = f1_score(y_test, predicted_labels_test, average='weighted')\n",
    "\n",
    "print(f'Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, predicted_labels_test)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4522c1a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.97642 | validation_accuracy: 0.40161 | test_accuracy: 0.40212 |  0:02:05s\n",
      "epoch 1  | loss: 1.62754 | validation_accuracy: 0.46586 | test_accuracy: 0.46533 |  0:04:10s\n",
      "epoch 2  | loss: 1.49843 | validation_accuracy: 0.50719 | test_accuracy: 0.50751 |  0:06:16s\n",
      "epoch 3  | loss: 1.45283 | validation_accuracy: 0.57678 | test_accuracy: 0.57787 |  0:08:20s\n",
      "epoch 4  | loss: 1.40475 | validation_accuracy: 0.52407 | test_accuracy: 0.52488 |  0:10:22s\n",
      "epoch 5  | loss: 1.35478 | validation_accuracy: 0.49926 | test_accuracy: 0.49917 |  0:12:27s\n",
      "epoch 6  | loss: 1.33864 | validation_accuracy: 0.60885 | test_accuracy: 0.60972 |  0:14:36s\n",
      "epoch 7  | loss: 1.31064 | validation_accuracy: 0.5449 | test_accuracy: 0.54486 |  0:16:45s\n",
      "epoch 8  | loss: 1.29692 | validation_accuracy: 0.51509 | test_accuracy: 0.51513 |  0:18:54s\n",
      "epoch 9  | loss: 1.28205 | validation_accuracy: 0.55115 | test_accuracy: 0.55073 |  0:21:03s\n",
      "epoch 10  | loss: 1.21888 | validation_accuracy: 0.59068 | test_accuracy: 0.59121 |  0:23:12s\n",
      "epoch 11  | loss: 1.19249 | validation_accuracy: 0.58401 | test_accuracy: 0.58455 |  0:25:21s\n",
      "epoch 12  | loss: 1.18433 | validation_accuracy: 0.49565 | test_accuracy: 0.49544 |  0:27:30s\n",
      "epoch 13  | loss: 1.19295 | validation_accuracy: 0.58502 | test_accuracy: 0.58469 |  0:29:40s\n",
      "epoch 14  | loss: 1.15824 | validation_accuracy: 0.63896 | test_accuracy: 0.63824 |  0:31:42s\n",
      "epoch 15  | loss: 1.13415 | validation_accuracy: 0.4464 | test_accuracy: 0.44596 |  0:33:43s\n",
      "epoch 16  | loss: 1.14401 | validation_accuracy: 0.56956 | test_accuracy: 0.56965 |  0:35:45s\n",
      "epoch 17  | loss: 1.10017 | validation_accuracy: 0.48627 | test_accuracy: 0.48569 |  0:37:51s\n",
      "epoch 18  | loss: 1.10438 | validation_accuracy: 0.66464 | test_accuracy: 0.66475 |  0:39:54s\n",
      "epoch 19  | loss: 1.07854 | validation_accuracy: 0.63639 | test_accuracy: 0.63662 |  0:41:55s\n",
      "epoch 20  | loss: 1.03224 | validation_accuracy: 0.68137 | test_accuracy: 0.68121 |  0:43:55s\n",
      "epoch 21  | loss: 1.03053 | validation_accuracy: 0.63097 | test_accuracy: 0.63128 |  0:45:54s\n",
      "epoch 22  | loss: 1.01342 | validation_accuracy: 0.66047 | test_accuracy: 0.66096 |  0:47:53s\n",
      "epoch 23  | loss: 0.99717 | validation_accuracy: 0.66332 | test_accuracy: 0.66321 |  0:49:52s\n",
      "epoch 24  | loss: 0.97746 | validation_accuracy: 0.66393 | test_accuracy: 0.6635 |  0:51:52s\n",
      "epoch 25  | loss: 0.97214 | validation_accuracy: 0.61041 | test_accuracy: 0.60952 |  0:53:51s\n",
      "epoch 26  | loss: 0.96 | validation_accuracy: 0.6775 | test_accuracy: 0.6772 |  0:55:52s\n",
      "epoch 27  | loss: 0.93969 | validation_accuracy: 0.64936 | test_accuracy: 0.64919 |  0:58:01s\n",
      "epoch 28  | loss: 0.94518 | validation_accuracy: 0.68236 | test_accuracy: 0.68198 |  1:00:10s\n",
      "epoch 29  | loss: 0.92697 | validation_accuracy: 0.57048 | test_accuracy: 0.57021 |  1:02:20s\n",
      "epoch 30  | loss: 0.90514 | validation_accuracy: 0.61408 | test_accuracy: 0.61394 |  1:04:30s\n",
      "epoch 31  | loss: 0.89765 | validation_accuracy: 0.53147 | test_accuracy: 0.53175 |  1:06:38s\n",
      "epoch 32  | loss: 0.88251 | validation_accuracy: 0.59885 | test_accuracy: 0.59838 |  1:08:44s\n",
      "epoch 33  | loss: 0.88086 | validation_accuracy: 0.5098 | test_accuracy: 0.51002 |  1:10:51s\n",
      "epoch 34  | loss: 0.87495 | validation_accuracy: 0.56924 | test_accuracy: 0.56876 |  1:12:57s\n",
      "epoch 35  | loss: 0.85775 | validation_accuracy: 0.56004 | test_accuracy: 0.55882 |  1:15:04s\n",
      "epoch 36  | loss: 0.85606 | validation_accuracy: 0.51862 | test_accuracy: 0.51786 |  1:17:10s\n",
      "epoch 37  | loss: 0.8371 | validation_accuracy: 0.77204 | test_accuracy: 0.77111 |  1:19:17s\n",
      "epoch 38  | loss: 0.85481 | validation_accuracy: 0.65821 | test_accuracy: 0.65826 |  1:21:22s\n",
      "epoch 39  | loss: 0.83519 | validation_accuracy: 0.65814 | test_accuracy: 0.65814 |  1:23:26s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 37 and best_test_accuracy = 0.77111\n",
      "Accuracy: 77.11\n",
      "Precision: 77.29\n",
      "Recall: 77.11\n",
      "F1-Score: 76.74\n",
      "Overall TP: 77.08\n",
      "Overall TN: 76.33\n",
      "Overall FP: 11.83\n",
      "Overall FN: 11.83\n"
     ]
    }
   ],
   "source": [
    "#TabNet-A\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "     '1-All-Together-Update-Single-26-8-24.csv',\n",
    "   '2-All-Together-Update-Single-26-8-24.csv',\n",
    "    '3-All-Together-Update-Single-26-8-24.csv',\n",
    "    '4-All-Together-Update-Single-26-8-24.csv',\n",
    "   '5-All-Together-Update-Single-26-8-24.csv'\n",
    "   \n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Drop 'Source IP' and 'Destination IP' columns\n",
    "data = data.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Convert categorical 'Protocol' column to numerical using Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = data.drop(columns=['Label'])  # Assuming 'Label' is your target column\n",
    "y = data['Label']\n",
    "\n",
    "# Step 1: Split into Training (80%) and Testing (20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Further split Training into Training (80%) and Validation (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val)\n",
    "\n",
    "# Define the TabNet model with fine-tuned hyperparameters\n",
    "tabnet_model = TabNetClassifier(\n",
    "    n_d=32,  # Reduced Decision layer output size\n",
    "    n_a=32,  # Reduced Attention layer size\n",
    "    n_steps=3,  # Reduced number of hidden layers (decision steps)\n",
    "    gamma=1.0,  # Adjusted coefficient for feature reusage in decision step\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-2),  # Reduced learning rate\n",
    "    scheduler_params={\"step_size\":20, \"gamma\":0.8},  # Adjusted learning rate scheduler\n",
    "    mask_type='entmax',  # Changed mask type in TabNet\n",
    ")\n",
    "\n",
    "# Train the TabNet model\n",
    "tabnet_model.fit(\n",
    "    X_train=X_train.values, y_train=y_train.values,\n",
    "    eval_set=[(X_val.values, y_val.values), (X_test.values, y_test.values)],\n",
    "    eval_name=['validation', 'test'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=40,  # Increase number of epochs\n",
    "    patience=10,  # Patience for early stopping\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=1, drop_last=False\n",
    ")\n",
    "\n",
    "# Evaluate the model on Validation and Test sets\n",
    "val_preds = tabnet_model.predict(X_val.values)\n",
    "test_preds = tabnet_model.predict(X_test.values)\n",
    "\n",
    "# Calculate metrics for Validation set\n",
    "val_accuracy = accuracy_score(y_val, val_preds)\n",
    "val_precision = precision_score(y_val, val_preds, average='weighted')\n",
    "val_recall = recall_score(y_val, val_preds, average='weighted')\n",
    "val_f1 = f1_score(y_val, val_preds, average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "# Calculate metrics for Test set\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "test_precision = precision_score(y_test, test_preds, average='weighted')\n",
    "test_recall = recall_score(y_test, test_preds, average='weighted')\n",
    "test_f1 = f1_score(y_test, test_preds, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {test_precision * 100:.2f}%\")\n",
    "print(f\"Recall: {test_recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {test_f1 * 100:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, val_preds)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a436ef0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.1253 | validation_accuracy: 0.34256 | test_accuracy: 0.34316 |  0:01:03s\n",
      "epoch 1  | loss: 1.81025 | validation_accuracy: 0.42152 | test_accuracy: 0.4219 |  0:02:06s\n",
      "epoch 2  | loss: 1.64385 | validation_accuracy: 0.4915 | test_accuracy: 0.49245 |  0:03:09s\n",
      "epoch 3  | loss: 1.55703 | validation_accuracy: 0.47667 | test_accuracy: 0.47763 |  0:04:12s\n",
      "epoch 4  | loss: 1.49588 | validation_accuracy: 0.47285 | test_accuracy: 0.47316 |  0:05:15s\n",
      "epoch 5  | loss: 1.45646 | validation_accuracy: 0.55168 | test_accuracy: 0.55193 |  0:06:18s\n",
      "epoch 6  | loss: 1.42892 | validation_accuracy: 0.55157 | test_accuracy: 0.5521 |  0:07:21s\n",
      "epoch 7  | loss: 1.39888 | validation_accuracy: 0.57106 | test_accuracy: 0.57288 |  0:08:24s\n",
      "epoch 8  | loss: 1.35797 | validation_accuracy: 0.57327 | test_accuracy: 0.57382 |  0:09:29s\n",
      "epoch 9  | loss: 1.33848 | validation_accuracy: 0.56052 | test_accuracy: 0.56245 |  0:10:32s\n",
      "epoch 10  | loss: 1.29554 | validation_accuracy: 0.6163 | test_accuracy: 0.61725 |  0:11:35s\n",
      "epoch 11  | loss: 1.26687 | validation_accuracy: 0.52854 | test_accuracy: 0.53044 |  0:12:38s\n",
      "epoch 12  | loss: 1.25996 | validation_accuracy: 0.58133 | test_accuracy: 0.58138 |  0:13:42s\n",
      "epoch 13  | loss: 1.22828 | validation_accuracy: 0.54311 | test_accuracy: 0.54282 |  0:14:45s\n",
      "epoch 14  | loss: 1.20805 | validation_accuracy: 0.54258 | test_accuracy: 0.54476 |  0:15:48s\n",
      "epoch 15  | loss: 1.18625 | validation_accuracy: 0.59107 | test_accuracy: 0.59197 |  0:16:51s\n",
      "epoch 16  | loss: 1.16057 | validation_accuracy: 0.54525 | test_accuracy: 0.54445 |  0:17:53s\n",
      "epoch 17  | loss: 1.14635 | validation_accuracy: 0.65283 | test_accuracy: 0.65284 |  0:18:56s\n",
      "epoch 18  | loss: 1.1285 | validation_accuracy: 0.66317 | test_accuracy: 0.66328 |  0:20:00s\n",
      "epoch 19  | loss: 1.11568 | validation_accuracy: 0.62487 | test_accuracy: 0.626 |  0:21:02s\n",
      "epoch 20  | loss: 1.07384 | validation_accuracy: 0.61398 | test_accuracy: 0.61428 |  0:22:05s\n",
      "epoch 21  | loss: 1.06084 | validation_accuracy: 0.63787 | test_accuracy: 0.63992 |  0:23:09s\n",
      "epoch 22  | loss: 1.06212 | validation_accuracy: 0.66963 | test_accuracy: 0.67059 |  0:24:12s\n",
      "epoch 23  | loss: 1.04247 | validation_accuracy: 0.70046 | test_accuracy: 0.70135 |  0:25:15s\n",
      "epoch 24  | loss: 1.02875 | validation_accuracy: 0.59209 | test_accuracy: 0.59355 |  0:26:18s\n",
      "epoch 25  | loss: 1.01448 | validation_accuracy: 0.53272 | test_accuracy: 0.53334 |  0:27:21s\n",
      "epoch 26  | loss: 1.00771 | validation_accuracy: 0.56855 | test_accuracy: 0.56889 |  0:28:24s\n",
      "epoch 27  | loss: 0.99213 | validation_accuracy: 0.61668 | test_accuracy: 0.61733 |  0:29:27s\n",
      "epoch 28  | loss: 0.97723 | validation_accuracy: 0.60289 | test_accuracy: 0.60381 |  0:30:30s\n",
      "epoch 29  | loss: 0.97683 | validation_accuracy: 0.6928 | test_accuracy: 0.69231 |  0:31:34s\n",
      "epoch 30  | loss: 0.93598 | validation_accuracy: 0.50156 | test_accuracy: 0.5026 |  0:32:37s\n",
      "epoch 31  | loss: 0.93175 | validation_accuracy: 0.60945 | test_accuracy: 0.60981 |  0:33:40s\n",
      "epoch 32  | loss: 1.02818 | validation_accuracy: 0.59522 | test_accuracy: 0.59585 |  0:34:43s\n",
      "epoch 33  | loss: 0.95281 | validation_accuracy: 0.71102 | test_accuracy: 0.71132 |  0:35:46s\n",
      "epoch 34  | loss: 0.92985 | validation_accuracy: 0.58352 | test_accuracy: 0.58437 |  0:36:49s\n",
      "epoch 35  | loss: 0.9131 | validation_accuracy: 0.60306 | test_accuracy: 0.60289 |  0:37:52s\n",
      "epoch 36  | loss: 0.8987 | validation_accuracy: 0.55873 | test_accuracy: 0.5596 |  0:38:55s\n",
      "epoch 37  | loss: 0.89569 | validation_accuracy: 0.60934 | test_accuracy: 0.60948 |  0:39:58s\n",
      "epoch 38  | loss: 0.8864 | validation_accuracy: 0.63941 | test_accuracy: 0.63939 |  0:41:01s\n",
      "epoch 39  | loss: 0.8828 | validation_accuracy: 0.69997 | test_accuracy: 0.69945 |  0:42:05s\n",
      "Stop training because you reached max_epochs = 40 with best_epoch = 33 and best_test_accuracy = 0.71132\n",
      "Accuracy: 71.13\n",
      "Precision: 71.04\n",
      "Recall: 71.13\n",
      "F1-Score: 70.91\n",
      "Overall TP: 71.25\n",
      "Overall TN: 70.62\n",
      "Overall FP: 3.55\n",
      "Overall FN: 3.55\n"
     ]
    }
   ],
   "source": [
    "#TabNet-B\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "     '1-Update-3-All_together-Bi.csv',\n",
    "    '2-Update-3-All_together-Bi.csv',\n",
    "    '3-Update-3-All_together-Bi.csv'\n",
    "   \n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Drop 'Source IP' and 'Destination IP' columns\n",
    "data = data.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Convert categorical 'Protocol' column to numerical using Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = data.drop(columns=['Label'])  # Assuming 'Label' is your target column\n",
    "y = data['Label']\n",
    "\n",
    "# Step 1: Split into Training (80%) and Testing (20%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 2: Further split Training into Training (80%) and Validation (20%)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val)\n",
    "\n",
    "# Define the TabNet model with fine-tuned hyperparameters\n",
    "tabnet_model = TabNetClassifier(\n",
    "    n_d=32,  # Reduced Decision layer output size\n",
    "    n_a=32,  # Reduced Attention layer size\n",
    "    n_steps=3,  # Reduced number of hidden layers (decision steps)\n",
    "    gamma=1.0,  # Adjusted coefficient for feature reusage in decision step\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-2),  # Reduced learning rate\n",
    "    scheduler_params={\"step_size\":20, \"gamma\":0.8},  # Adjusted learning rate scheduler\n",
    "    mask_type='entmax',  # Changed mask type in TabNet\n",
    ")\n",
    "\n",
    "# Train the TabNet model\n",
    "tabnet_model.fit(\n",
    "    X_train=X_train.values, y_train=y_train.values,\n",
    "    eval_set=[(X_val.values, y_val.values), (X_test.values, y_test.values)],\n",
    "    eval_name=['validation', 'test'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=40,  # Increase number of epochs\n",
    "    patience=10,  # Patience for early stopping\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=1, drop_last=False\n",
    ")\n",
    "\n",
    "# Evaluate the model on Validation and Test sets\n",
    "val_preds = tabnet_model.predict(X_val.values)\n",
    "test_preds = tabnet_model.predict(X_test.values)\n",
    "\n",
    "# Calculate metrics for Validation set\n",
    "val_accuracy = accuracy_score(y_val, val_preds)\n",
    "val_precision = precision_score(y_val, val_preds, average='weighted')\n",
    "val_recall = recall_score(y_val, val_preds, average='weighted')\n",
    "val_f1 = f1_score(y_val, val_preds, average='weighted')\n",
    "\n",
    "\n",
    "\n",
    "# Calculate metrics for Test set\n",
    "test_accuracy = accuracy_score(y_test, test_preds)\n",
    "test_precision = precision_score(y_test, test_preds, average='weighted')\n",
    "test_recall = recall_score(y_test, test_preds, average='weighted')\n",
    "test_f1 = f1_score(y_test, test_preds, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {test_accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {test_precision * 100:.2f}%\")\n",
    "print(f\"Recall: {test_recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {test_f1 * 100:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_val, val_preds)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b50f11cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 1.93248 | validation_accuracy: 0.1773 |  0:01:04s\n",
      "epoch 1  | loss: 1.64357 | validation_accuracy: 0.1681 |  0:02:09s\n",
      "epoch 2  | loss: 1.51891 | validation_accuracy: 0.1606 |  0:03:18s\n",
      "epoch 3  | loss: 1.44765 | validation_accuracy: 0.16881 |  0:04:32s\n",
      "epoch 4  | loss: 1.39207 | validation_accuracy: 0.15698 |  0:05:36s\n",
      "epoch 5  | loss: 1.35884 | validation_accuracy: 0.16609 |  0:06:41s\n",
      "epoch 6  | loss: 1.31611 | validation_accuracy: 0.14968 |  0:07:45s\n",
      "epoch 7  | loss: 1.30148 | validation_accuracy: 0.1519 |  0:08:49s\n",
      "epoch 8  | loss: 1.26737 | validation_accuracy: 0.14716 |  0:09:54s\n",
      "epoch 9  | loss: 1.23795 | validation_accuracy: 0.15809 |  0:10:58s\n",
      "epoch 10  | loss: 1.21772 | validation_accuracy: 0.1775 |  0:12:02s\n",
      "epoch 11  | loss: 1.19136 | validation_accuracy: 0.16083 |  0:13:07s\n",
      "epoch 12  | loss: 1.17117 | validation_accuracy: 0.16557 |  0:14:12s\n",
      "epoch 13  | loss: 1.15543 | validation_accuracy: 0.14526 |  0:15:17s\n",
      "epoch 14  | loss: 1.12842 | validation_accuracy: 0.13801 |  0:16:22s\n",
      "epoch 15  | loss: 1.12645 | validation_accuracy: 0.14878 |  0:17:27s\n",
      "epoch 16  | loss: 1.08903 | validation_accuracy: 0.13035 |  0:18:32s\n",
      "epoch 17  | loss: 1.07094 | validation_accuracy: 0.14451 |  0:19:37s\n",
      "epoch 18  | loss: 1.06272 | validation_accuracy: 0.11167 |  0:20:42s\n",
      "epoch 19  | loss: 1.03929 | validation_accuracy: 0.13598 |  0:21:47s\n",
      "epoch 20  | loss: 1.02639 | validation_accuracy: 0.12712 |  0:22:52s\n",
      "Early stopping occurred at epoch 20 with best_epoch = 10 and best_validation_accuracy = 0.1775\n",
      "Accuracy: 98.49\n",
      "Precision: 98.49\n",
      "Recall: 98.49\n",
      "F1-Score: 98.49\n",
      "Overall TP: 98.43\n",
      "Overall TN: 97.71\n",
      "Overall FP: 0.76\n",
      "Overall FN: 0.76\n"
     ]
    }
   ],
   "source": [
    "##Original-TABNET-XGB-----------------------80-20\n",
    "#Tabnet-XGB-A\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-All-Together-Update-Single-26-8-24.csv',\n",
    "    '2-All-Together-Update-Single-26-8-24.csv',\n",
    "    '3-All-Together-Update-Single-26-8-24.csv',\n",
    "    '4-All-Together-Update-Single-26-8-24.csv',\n",
    "    '5-All-Together-Update-Single-26-8-24.csv'\n",
    "   # '1-Update-3-All_together-Bi.csv',\n",
    "   # '2-Update-3-All_together-Bi.csv',\n",
    "    #'3-Update-3-All_together-Bi.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Drop 'Source IP' and 'Destination IP' columns\n",
    "data = data.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Convert categorical 'Protocol' column to numerical using Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = data.drop(columns=['Label'])  # Assuming 'Label' is your target column\n",
    "y = data['Label']\n",
    "\n",
    "# First, split the dataset into 80% training and 20% testing sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, split the 80% training set into 80% actual training and 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the TabNet model with 4 decision steps (hidden layers)\n",
    "tabnet_model = TabNetClassifier(\n",
    "     n_d=32,  # Reduced Decision layer output size\n",
    "    n_a=32,  # Reduced Attention layer size\n",
    "    n_steps=3,  # Reduced number of hidden layers (decision steps)\n",
    "    gamma=1.0,  # Adjusted coefficient for feature reusage in decision step\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-2),  # Reduced learning rate\n",
    "    scheduler_params={\"step_size\":20, \"gamma\":0.8},  # Adjusted learning rate scheduler\n",
    "    mask_type='entmax',  # Changed mask type in TabNet\n",
    ")\n",
    "\n",
    "# Train the TabNet model with the validation set\n",
    "tabnet_model.fit(\n",
    "    X_train=X_train.values, y_train=y_train.values,\n",
    "    eval_set=[(X_val.values, y_val.values)],\n",
    "    eval_name=['validation'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=40,  # Set the number of epochs\n",
    "    patience=10,  # Stop if no improvement after 10 epochs\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=1, drop_last=False\n",
    ")\n",
    "\n",
    "# Get the probability scores from TabNet\n",
    "tabnet_train_output = tabnet_model.predict_proba(X_train.values)\n",
    "tabnet_val_output = tabnet_model.predict_proba(X_val.values)\n",
    "tabnet_test_output = tabnet_model.predict_proba(X_test.values)\n",
    "\n",
    "# Train an XGBoost model on TabNet's training output\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(tabnet_train_output, y_train)\n",
    "\n",
    "# Predict with the XGBoost model on validation and test sets\n",
    "xgb_val_preds = xgb_model.predict(tabnet_val_output)\n",
    "xgb_test_preds = xgb_model.predict(tabnet_test_output)\n",
    "\n",
    "# Evaluate XGBoost model performance on the testing set\n",
    "xgb_test_accuracy = accuracy_score(y_test, xgb_test_preds)\n",
    "xgb_test_precision = precision_score(y_test, xgb_test_preds, average='weighted')\n",
    "xgb_test_recall = recall_score(y_test, xgb_test_preds, average='weighted')\n",
    "xgb_test_f1 = f1_score(y_test, xgb_test_preds, average='weighted')\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {xgb_test_accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {xgb_test_precisionn * 100:.2f}%\")\n",
    "print(f\"Recall: {xgb_test_recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {xgb_test_f1 * 100:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, xgb_test_preds)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e4c1b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 2.13592 | validation_accuracy: 0.18737 |  0:00:32s\n",
      "epoch 1  | loss: 1.84975 | validation_accuracy: 0.18929 |  0:01:03s\n",
      "epoch 2  | loss: 1.70977 | validation_accuracy: 0.17264 |  0:01:35s\n",
      "epoch 3  | loss: 1.60026 | validation_accuracy: 0.17847 |  0:02:07s\n",
      "epoch 4  | loss: 1.53786 | validation_accuracy: 0.18306 |  0:02:38s\n",
      "epoch 5  | loss: 1.49343 | validation_accuracy: 0.15583 |  0:03:10s\n",
      "epoch 6  | loss: 1.46453 | validation_accuracy: 0.14998 |  0:03:42s\n",
      "epoch 7  | loss: 1.44388 | validation_accuracy: 0.14958 |  0:04:13s\n",
      "epoch 8  | loss: 1.39924 | validation_accuracy: 0.16248 |  0:04:45s\n",
      "epoch 9  | loss: 1.39855 | validation_accuracy: 0.1412 |  0:05:16s\n",
      "epoch 10  | loss: 1.37862 | validation_accuracy: 0.15747 |  0:05:48s\n",
      "epoch 11  | loss: 1.34189 | validation_accuracy: 0.13419 |  0:06:20s\n",
      "Early stopping occurred at epoch 11 with best_epoch = 1 and best_validation_accuracy = 0.18929\n",
      "Accuracy: 96.58\n",
      "Precision: 96.58\n",
      "Recall: 96.58\n",
      "F1-Score: 96.58\n",
      "Overall TP: 96.56\n",
      "Overall TN: 95.92\n",
      "Overall FP: 1.04\n",
      "Overall FN: 1.04\n"
     ]
    }
   ],
   "source": [
    "##Original-TABNET-XGB-----------------------80-20\n",
    "#Tabnet-XGB-B\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    \n",
    "    '1-Update-3-All_together-Bi.csv',\n",
    "    '2-Update-3-All_together-Bi.csv',\n",
    "    '3-Update-3-All_together-Bi.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Drop 'Source IP' and 'Destination IP' columns\n",
    "data = data.drop(columns=['Source IP', 'Destination IP'])\n",
    "\n",
    "# Convert categorical 'Protocol' column to numerical using Label Encoding\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = data.drop(columns=['Label'])  # Assuming 'Label' is your target column\n",
    "y = data['Label']\n",
    "\n",
    "# First, split the dataset into 80% training and 20% testing sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Then, split the 80% training set into 80% actual training and 20% validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the TabNet model with 4 decision steps (hidden layers)\n",
    "tabnet_model = TabNetClassifier(\n",
    "     n_d=32,  # Reduced Decision layer output size\n",
    "    n_a=32,  # Reduced Attention layer size\n",
    "    n_steps=3,  # Reduced number of hidden layers (decision steps)\n",
    "    gamma=1.0,  # Adjusted coefficient for feature reusage in decision step\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-2),  # Reduced learning rate\n",
    "    scheduler_params={\"step_size\":20, \"gamma\":0.8},  # Adjusted learning rate scheduler\n",
    "    mask_type='entmax',  # Changed mask type in TabNet\n",
    ")\n",
    "\n",
    "# Train the TabNet model with the validation set\n",
    "tabnet_model.fit(\n",
    "    X_train=X_train.values, y_train=y_train.values,\n",
    "    eval_set=[(X_val.values, y_val.values)],\n",
    "    eval_name=['validation'],\n",
    "    eval_metric=['accuracy'],\n",
    "    max_epochs=40,  # Set the number of epochs\n",
    "    patience=10,  # Stop if no improvement after 10 epochs\n",
    "    batch_size=1024, virtual_batch_size=128,\n",
    "    num_workers=1, drop_last=False\n",
    ")\n",
    "\n",
    "# Get the probability scores from TabNet\n",
    "tabnet_train_output = tabnet_model.predict_proba(X_train.values)\n",
    "tabnet_val_output = tabnet_model.predict_proba(X_val.values)\n",
    "tabnet_test_output = tabnet_model.predict_proba(X_test.values)\n",
    "\n",
    "# Train an XGBoost model on TabNet's training output\n",
    "xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "xgb_model.fit(tabnet_train_output, y_train)\n",
    "\n",
    "# Predict with the XGBoost model on validation and test sets\n",
    "xgb_val_preds = xgb_model.predict(tabnet_val_output)\n",
    "xgb_test_preds = xgb_model.predict(tabnet_test_output)\n",
    "\n",
    "# Evaluate XGBoost model performance on the testing set\n",
    "xgb_test_accuracy = accuracy_score(y_test, xgb_test_preds)\n",
    "xgb_test_precision = precision_score(y_test, xgb_test_preds, average='weighted')\n",
    "xgb_test_recall = recall_score(y_test, xgb_test_preds, average='weighted')\n",
    "xgb_test_f1 = f1_score(y_test, xgb_test_preds, average='weighted')\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {xgb_test_accuracy * 100:.2f}%\")\n",
    "print(f\"Precision: {xgb_test_precisionn * 100:.2f}%\")\n",
    "print(f\"Recall: {xgb_test_recall * 100:.2f}%\")\n",
    "print(f\"F1-Score: {xgb_test_f1 * 100:.2f}%\")\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, xgb_test_preds)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e8b2e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 965us/step - accuracy: 0.3306 - loss: 1.9533 - val_accuracy: 0.4981 - val_loss: 1.4610\n",
      "Epoch 2/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 962us/step - accuracy: 0.5270 - loss: 1.3876 - val_accuracy: 0.5793 - val_loss: 1.2307\n",
      "Epoch 3/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 965us/step - accuracy: 0.5973 - loss: 1.1830 - val_accuracy: 0.6269 - val_loss: 1.0949\n",
      "Epoch 4/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 962us/step - accuracy: 0.6396 - loss: 1.0609 - val_accuracy: 0.6512 - val_loss: 1.0155\n",
      "Epoch 5/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 967us/step - accuracy: 0.6684 - loss: 0.9764 - val_accuracy: 0.6842 - val_loss: 0.9356\n",
      "Epoch 6/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 971us/step - accuracy: 0.6901 - loss: 0.9134 - val_accuracy: 0.7027 - val_loss: 0.8849\n",
      "Epoch 7/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 972us/step - accuracy: 0.7082 - loss: 0.8635 - val_accuracy: 0.7167 - val_loss: 0.8325\n",
      "Epoch 8/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 967us/step - accuracy: 0.7210 - loss: 0.8251 - val_accuracy: 0.7275 - val_loss: 0.7975\n",
      "Epoch 9/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 974us/step - accuracy: 0.7328 - loss: 0.7925 - val_accuracy: 0.7432 - val_loss: 0.7699\n",
      "Epoch 10/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 971us/step - accuracy: 0.7426 - loss: 0.7648 - val_accuracy: 0.7490 - val_loss: 0.7455\n",
      "Epoch 11/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 950us/step - accuracy: 0.7498 - loss: 0.7442 - val_accuracy: 0.7542 - val_loss: 0.7354\n",
      "Epoch 12/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 952us/step - accuracy: 0.7568 - loss: 0.7240 - val_accuracy: 0.7600 - val_loss: 0.7222\n",
      "Epoch 13/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 954us/step - accuracy: 0.7637 - loss: 0.7046 - val_accuracy: 0.7643 - val_loss: 0.7018\n",
      "Epoch 14/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 956us/step - accuracy: 0.7698 - loss: 0.6874 - val_accuracy: 0.7705 - val_loss: 0.6878\n",
      "Epoch 15/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 953us/step - accuracy: 0.7745 - loss: 0.6748 - val_accuracy: 0.7661 - val_loss: 0.6983\n",
      "Epoch 16/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 952us/step - accuracy: 0.7790 - loss: 0.6627 - val_accuracy: 0.7868 - val_loss: 0.6513\n",
      "Epoch 17/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 954us/step - accuracy: 0.7830 - loss: 0.6508 - val_accuracy: 0.7757 - val_loss: 0.6683\n",
      "Epoch 18/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 955us/step - accuracy: 0.7870 - loss: 0.6407 - val_accuracy: 0.7897 - val_loss: 0.6277\n",
      "Epoch 19/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 955us/step - accuracy: 0.7902 - loss: 0.6306 - val_accuracy: 0.7969 - val_loss: 0.6049\n",
      "Epoch 20/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 957us/step - accuracy: 0.7925 - loss: 0.6247 - val_accuracy: 0.7969 - val_loss: 0.6124\n",
      "Epoch 21/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 960us/step - accuracy: 0.7952 - loss: 0.6176 - val_accuracy: 0.8005 - val_loss: 0.6103\n",
      "Epoch 22/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 964us/step - accuracy: 0.7978 - loss: 0.6092 - val_accuracy: 0.7849 - val_loss: 0.6431\n",
      "Epoch 23/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 964us/step - accuracy: 0.8011 - loss: 0.6002 - val_accuracy: 0.7946 - val_loss: 0.6158\n",
      "Epoch 24/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 971us/step - accuracy: 0.8030 - loss: 0.5955 - val_accuracy: 0.8097 - val_loss: 0.5685\n",
      "Epoch 25/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 966us/step - accuracy: 0.8052 - loss: 0.5892 - val_accuracy: 0.8052 - val_loss: 0.5890\n",
      "Epoch 26/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 966us/step - accuracy: 0.8079 - loss: 0.5824 - val_accuracy: 0.7984 - val_loss: 0.6197\n",
      "Epoch 27/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 968us/step - accuracy: 0.8091 - loss: 0.5788 - val_accuracy: 0.8108 - val_loss: 0.5740\n",
      "Epoch 28/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 971us/step - accuracy: 0.8108 - loss: 0.5741 - val_accuracy: 0.8301 - val_loss: 0.5228\n",
      "Epoch 29/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 971us/step - accuracy: 0.8129 - loss: 0.5691 - val_accuracy: 0.8158 - val_loss: 0.5595\n",
      "Epoch 30/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 970us/step - accuracy: 0.8147 - loss: 0.5629 - val_accuracy: 0.8204 - val_loss: 0.5429\n",
      "Epoch 31/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 972us/step - accuracy: 0.8162 - loss: 0.5595 - val_accuracy: 0.8282 - val_loss: 0.5246\n",
      "Epoch 32/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 41s 974us/step - accuracy: 0.8175 - loss: 0.5558 - val_accuracy: 0.8153 - val_loss: 0.5576\n",
      "Epoch 33/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 40s 948us/step - accuracy: 0.8189 - loss: 0.5522 - val_accuracy: 0.8172 - val_loss: 0.5501\n",
      "Epoch 34/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 38s 903us/step - accuracy: 0.8194 - loss: 0.5507 - val_accuracy: 0.8239 - val_loss: 0.5510\n",
      "Epoch 35/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 38s 902us/step - accuracy: 0.8213 - loss: 0.5460 - val_accuracy: 0.8298 - val_loss: 0.5375\n",
      "Epoch 36/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 38s 900us/step - accuracy: 0.8223 - loss: 0.5419 - val_accuracy: 0.8292 - val_loss: 0.5256\n",
      "Epoch 37/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 38s 904us/step - accuracy: 0.8235 - loss: 0.5376 - val_accuracy: 0.8384 - val_loss: 0.5116\n",
      "Epoch 38/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 38s 903us/step - accuracy: 0.8250 - loss: 0.5331 - val_accuracy: 0.8141 - val_loss: 0.5607\n",
      "Epoch 39/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 38s 905us/step - accuracy: 0.8259 - loss: 0.5318 - val_accuracy: 0.8325 - val_loss: 0.5154\n",
      "Epoch 40/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 38s 905us/step - accuracy: 0.8267 - loss: 0.5290 - val_accuracy: 0.8151 - val_loss: 0.5568\n",
      "84356/84356 ━━━━━━━━━━━━━━━━━━━━ 38s 453us/step\n",
      "21089/21089 ━━━━━━━━━━━━━━━━━━━━ 11s 528us/step\n",
      "26362/26362 ━━━━━━━━━━━━━━━━━━━━ 14s 520us/step\n",
      "Epoch 1/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 147s 3ms/step - accuracy: 0.8135 - loss: 0.6689 - val_accuracy: 0.9060 - val_loss: 0.3195\n",
      "Epoch 2/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 143s 3ms/step - accuracy: 0.9150 - loss: 0.2892 - val_accuracy: 0.9330 - val_loss: 0.2331\n",
      "Epoch 3/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 143s 3ms/step - accuracy: 0.9357 - loss: 0.2248 - val_accuracy: 0.9414 - val_loss: 0.2057\n",
      "Epoch 4/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 141s 3ms/step - accuracy: 0.9430 - loss: 0.1996 - val_accuracy: 0.9460 - val_loss: 0.1922\n",
      "Epoch 5/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 141s 3ms/step - accuracy: 0.9469 - loss: 0.1865 - val_accuracy: 0.9491 - val_loss: 0.1804\n",
      "Epoch 6/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 142s 3ms/step - accuracy: 0.9493 - loss: 0.1774 - val_accuracy: 0.9495 - val_loss: 0.1788\n",
      "Epoch 7/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 141s 3ms/step - accuracy: 0.9509 - loss: 0.1711 - val_accuracy: 0.9494 - val_loss: 0.1777\n",
      "Epoch 8/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 150s 4ms/step - accuracy: 0.9521 - loss: 0.1664 - val_accuracy: 0.9543 - val_loss: 0.1645\n",
      "Epoch 9/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 160s 4ms/step - accuracy: 0.9535 - loss: 0.1617 - val_accuracy: 0.9545 - val_loss: 0.1615\n",
      "Epoch 10/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 160s 4ms/step - accuracy: 0.9541 - loss: 0.1593 - val_accuracy: 0.9532 - val_loss: 0.1634\n",
      "Epoch 11/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 160s 4ms/step - accuracy: 0.9550 - loss: 0.1557 - val_accuracy: 0.9549 - val_loss: 0.1612\n",
      "Epoch 12/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 160s 4ms/step - accuracy: 0.9558 - loss: 0.1535 - val_accuracy: 0.9565 - val_loss: 0.1554\n",
      "Epoch 13/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 159s 4ms/step - accuracy: 0.9562 - loss: 0.1514 - val_accuracy: 0.9556 - val_loss: 0.1601\n",
      "Epoch 14/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 160s 4ms/step - accuracy: 0.9568 - loss: 0.1500 - val_accuracy: 0.9569 - val_loss: 0.1522\n",
      "Epoch 15/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 159s 4ms/step - accuracy: 0.9569 - loss: 0.1491 - val_accuracy: 0.9563 - val_loss: 0.1541\n",
      "Epoch 16/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 158s 4ms/step - accuracy: 0.9576 - loss: 0.1471 - val_accuracy: 0.9571 - val_loss: 0.1531\n",
      "Epoch 17/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 158s 4ms/step - accuracy: 0.9580 - loss: 0.1456 - val_accuracy: 0.9581 - val_loss: 0.1503\n",
      "Epoch 18/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 158s 4ms/step - accuracy: 0.9583 - loss: 0.1446 - val_accuracy: 0.9565 - val_loss: 0.1529\n",
      "Epoch 19/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 155s 4ms/step - accuracy: 0.9586 - loss: 0.1432 - val_accuracy: 0.9576 - val_loss: 0.1505\n",
      "Epoch 20/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 143s 3ms/step - accuracy: 0.9589 - loss: 0.1423 - val_accuracy: 0.9572 - val_loss: 0.1516\n",
      "Epoch 21/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 146s 3ms/step - accuracy: 0.9593 - loss: 0.1414 - val_accuracy: 0.9586 - val_loss: 0.1479\n",
      "Epoch 22/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 147s 3ms/step - accuracy: 0.9597 - loss: 0.1403 - val_accuracy: 0.9585 - val_loss: 0.1480\n",
      "Epoch 23/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 147s 3ms/step - accuracy: 0.9596 - loss: 0.1398 - val_accuracy: 0.9595 - val_loss: 0.1465\n",
      "Epoch 24/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 147s 3ms/step - accuracy: 0.9601 - loss: 0.1384 - val_accuracy: 0.9584 - val_loss: 0.1497\n",
      "Epoch 25/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 147s 3ms/step - accuracy: 0.9602 - loss: 0.1385 - val_accuracy: 0.9590 - val_loss: 0.1477\n",
      "Epoch 26/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 147s 3ms/step - accuracy: 0.9604 - loss: 0.1377 - val_accuracy: 0.9593 - val_loss: 0.1458\n",
      "Epoch 27/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 149s 4ms/step - accuracy: 0.9603 - loss: 0.1373 - val_accuracy: 0.9589 - val_loss: 0.1467\n",
      "Epoch 28/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 149s 4ms/step - accuracy: 0.9607 - loss: 0.1368 - val_accuracy: 0.9580 - val_loss: 0.1500\n",
      "Epoch 29/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 149s 4ms/step - accuracy: 0.9611 - loss: 0.1353 - val_accuracy: 0.9602 - val_loss: 0.1436\n",
      "Epoch 30/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 150s 4ms/step - accuracy: 0.9610 - loss: 0.1352 - val_accuracy: 0.9604 - val_loss: 0.1431\n",
      "Epoch 31/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 149s 4ms/step - accuracy: 0.9612 - loss: 0.1348 - val_accuracy: 0.9604 - val_loss: 0.1435\n",
      "Epoch 32/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 149s 4ms/step - accuracy: 0.9618 - loss: 0.1334 - val_accuracy: 0.9608 - val_loss: 0.1418\n",
      "Epoch 33/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 149s 4ms/step - accuracy: 0.9614 - loss: 0.1342 - val_accuracy: 0.9599 - val_loss: 0.1422\n",
      "Epoch 34/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 149s 4ms/step - accuracy: 0.9615 - loss: 0.1338 - val_accuracy: 0.9608 - val_loss: 0.1409\n",
      "Epoch 35/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 150s 4ms/step - accuracy: 0.9617 - loss: 0.1330 - val_accuracy: 0.9597 - val_loss: 0.1437\n",
      "Epoch 36/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 150s 4ms/step - accuracy: 0.9617 - loss: 0.1323 - val_accuracy: 0.9600 - val_loss: 0.1434\n",
      "Epoch 37/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 151s 4ms/step - accuracy: 0.9620 - loss: 0.1318 - val_accuracy: 0.9609 - val_loss: 0.1411\n",
      "Epoch 38/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 151s 4ms/step - accuracy: 0.9619 - loss: 0.1318 - val_accuracy: 0.9608 - val_loss: 0.1414\n",
      "Epoch 39/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 150s 4ms/step - accuracy: 0.9620 - loss: 0.1317 - val_accuracy: 0.9618 - val_loss: 0.1391\n",
      "Epoch 40/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 151s 4ms/step - accuracy: 0.9621 - loss: 0.1315 - val_accuracy: 0.9605 - val_loss: 0.1411\n",
      "21089/21089 ━━━━━━━━━━━━━━━━━━━━ 23s 1ms/step - accuracy: 0.9606 - loss: 0.1409\n",
      "26362/26362 ━━━━━━━━━━━━━━━━━━━━ 29s 1ms/step - accuracy: 0.9606 - loss: 0.1407\n",
      "Test Accuracy: 96.0731%\n",
      "21089/21089 ━━━━━━━━━━━━━━━━━━━━ 22s 1ms/step\n",
      "26362/26362 ━━━━━━━━━━━━━━━━━━━━ 27s 1ms/step\n",
      "Accuracy: 96.07\n",
      "Precision: 96.09\n",
      "Recall: 96.07\n",
      "F1-Score: 96.07\n",
      "Overall TP: 96.08\n",
      "Overall TN: 95.37\n",
      "Overall FP: 1.96\n",
      "Overall FN: 1.96\n"
     ]
    }
   ],
   "source": [
    "#CNN-LSTM-A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    #'1-Update-3-All_together-Bi.csv',\n",
    "   # '2-Update-3-All_together-Bi.csv',\n",
    "    #'3-Update-3-All_together-Bi.csv',\n",
    "    '1-All-Together-Update-Single-26-8-24.csv',\n",
    "    '2-All-Together-Update-Single-26-8-24.csv',\n",
    "    '3-All-Together-Update-Single-26-8-24.csv',\n",
    "    '4-All-Together-Update-Single-26-8-24.csv',\n",
    "    '5-All-Together-Update-Single-26-8-24.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder_protocol = LabelEncoder()\n",
    "data['Protocol'] = label_encoder_protocol.fit_transform(data['Protocol'])\n",
    "\n",
    "label_encoder_label = LabelEncoder()\n",
    "data['Label'] = label_encoder_label.fit_transform(data['Label'])\n",
    "\n",
    "# Store the labels and drop unnecessary columns\n",
    "labels = data['Label'].values\n",
    "data.drop(columns=['Source IP', 'Destination IP', 'Label'], inplace=True)\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the 80% training data into training (80%) and validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training, validation, and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data for Conv1D layers\n",
    "X_train_final = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_val_final = X_val_scaled.reshape((X_val_scaled.shape[0], X_val_scaled.shape[1], 1))\n",
    "X_test_final = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Define the CNN base model\n",
    "cnn_model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train_final.shape[1], 1)),\n",
    "    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(np.unique(labels)), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the CNN model\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN model with validation data\n",
    "cnn_model.fit(\n",
    "    X_train_final, y_train,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val_final, y_val)\n",
    ")\n",
    "\n",
    "# Generate predictions for stacking\n",
    "cnn_train_predictions = cnn_model.predict(X_train_final)\n",
    "cnn_val_predictions = cnn_model.predict(X_val_final)\n",
    "cnn_test_predictions = cnn_model.predict(X_test_final)\n",
    "\n",
    "# Use the predicted probabilities as features for the LSTM model\n",
    "cnn_train_preds_for_lstm = cnn_train_predictions.reshape(cnn_train_predictions.shape[0], cnn_train_predictions.shape[1], 1)\n",
    "cnn_val_preds_for_lstm = cnn_val_predictions.reshape(cnn_val_predictions.shape[0], cnn_val_predictions.shape[1], 1)\n",
    "cnn_test_preds_for_lstm = cnn_test_predictions.reshape(cnn_test_predictions.shape[0], cnn_test_predictions.shape[1], 1)\n",
    "\n",
    "# Define the LSTM meta model\n",
    "lstm_model = keras.Sequential([\n",
    "    layers.LSTM(64, input_shape=(cnn_train_preds_for_lstm.shape[1], cnn_train_preds_for_lstm.shape[2])),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(len(np.unique(labels)), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the LSTM meta model\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM meta model with validation data\n",
    "lstm_model.fit(\n",
    "    cnn_train_preds_for_lstm, y_train,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_data=(cnn_val_preds_for_lstm, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the LSTM model on the validation and test sets\n",
    "val_loss, val_accuracy = lstm_model.evaluate(cnn_val_preds_for_lstm, y_val)\n",
    "test_loss, test_accuracy = lstm_model.evaluate(cnn_test_preds_for_lstm, y_test)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.4f}%')\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.4f}%')\n",
    "\n",
    "# Make predictions on the validation and test sets using the stacked model\n",
    "val_predictions = lstm_model.predict(cnn_val_preds_for_lstm)\n",
    "test_predictions = lstm_model.predict(cnn_test_preds_for_lstm)\n",
    "\n",
    "# Calculate predicted labels for validation and test sets\n",
    "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_accuracy = accuracy_score(y_test, test_predicted_labels)\n",
    "test_precision = precision_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "test_f1_score = f1_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "\n",
    "print(f'Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predicted_labels)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a52db2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 898us/step - accuracy: 0.2944 - loss: 2.0591 - val_accuracy: 0.4690 - val_loss: 1.5592\n",
      "Epoch 2/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 893us/step - accuracy: 0.5079 - loss: 1.4457 - val_accuracy: 0.5957 - val_loss: 1.1980\n",
      "Epoch 3/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 897us/step - accuracy: 0.6036 - loss: 1.1606 - val_accuracy: 0.6470 - val_loss: 1.0274\n",
      "Epoch 4/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 882us/step - accuracy: 0.6591 - loss: 0.9982 - val_accuracy: 0.6870 - val_loss: 0.9128\n",
      "Epoch 5/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 884us/step - accuracy: 0.6951 - loss: 0.8992 - val_accuracy: 0.7190 - val_loss: 0.8310\n",
      "Epoch 6/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 894us/step - accuracy: 0.7219 - loss: 0.8219 - val_accuracy: 0.7359 - val_loss: 0.7833\n",
      "Epoch 7/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 889us/step - accuracy: 0.7428 - loss: 0.7628 - val_accuracy: 0.7426 - val_loss: 0.7492\n",
      "Epoch 8/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 896us/step - accuracy: 0.7586 - loss: 0.7172 - val_accuracy: 0.7654 - val_loss: 0.6920\n",
      "Epoch 9/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 932us/step - accuracy: 0.7710 - loss: 0.6805 - val_accuracy: 0.7858 - val_loss: 0.6353\n",
      "Epoch 10/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 21s 1ms/step - accuracy: 0.7819 - loss: 0.6488 - val_accuracy: 0.7932 - val_loss: 0.6191\n",
      "Epoch 11/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 21s 1ms/step - accuracy: 0.7914 - loss: 0.6220 - val_accuracy: 0.8010 - val_loss: 0.6003\n",
      "Epoch 12/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 21s 1ms/step - accuracy: 0.7987 - loss: 0.6003 - val_accuracy: 0.8075 - val_loss: 0.5715\n",
      "Epoch 13/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 21s 1ms/step - accuracy: 0.8046 - loss: 0.5842 - val_accuracy: 0.8156 - val_loss: 0.5676\n",
      "Epoch 14/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 980us/step - accuracy: 0.8103 - loss: 0.5669 - val_accuracy: 0.8213 - val_loss: 0.5548\n",
      "Epoch 15/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 20s 958us/step - accuracy: 0.8148 - loss: 0.5545 - val_accuracy: 0.8122 - val_loss: 0.5533\n",
      "Epoch 16/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 914us/step - accuracy: 0.8201 - loss: 0.5392 - val_accuracy: 0.8242 - val_loss: 0.5379\n",
      "Epoch 17/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 896us/step - accuracy: 0.8245 - loss: 0.5280 - val_accuracy: 0.8237 - val_loss: 0.5255\n",
      "Epoch 18/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 894us/step - accuracy: 0.8281 - loss: 0.5171 - val_accuracy: 0.8278 - val_loss: 0.5326\n",
      "Epoch 19/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 901us/step - accuracy: 0.8326 - loss: 0.5035 - val_accuracy: 0.8506 - val_loss: 0.4658\n",
      "Epoch 20/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 903us/step - accuracy: 0.8361 - loss: 0.4952 - val_accuracy: 0.8508 - val_loss: 0.4647\n",
      "Epoch 21/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 901us/step - accuracy: 0.8393 - loss: 0.4846 - val_accuracy: 0.8443 - val_loss: 0.4768\n",
      "Epoch 22/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 902us/step - accuracy: 0.8422 - loss: 0.4767 - val_accuracy: 0.8475 - val_loss: 0.4612\n",
      "Epoch 23/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 903us/step - accuracy: 0.8469 - loss: 0.4634 - val_accuracy: 0.8292 - val_loss: 0.5119\n",
      "Epoch 24/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 901us/step - accuracy: 0.8486 - loss: 0.4581 - val_accuracy: 0.8457 - val_loss: 0.4678\n",
      "Epoch 25/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 905us/step - accuracy: 0.8521 - loss: 0.4460 - val_accuracy: 0.8583 - val_loss: 0.4312\n",
      "Epoch 26/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 902us/step - accuracy: 0.8539 - loss: 0.4394 - val_accuracy: 0.8417 - val_loss: 0.4706\n",
      "Epoch 27/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 904us/step - accuracy: 0.8563 - loss: 0.4355 - val_accuracy: 0.8610 - val_loss: 0.4243\n",
      "Epoch 28/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 905us/step - accuracy: 0.8583 - loss: 0.4276 - val_accuracy: 0.8472 - val_loss: 0.4527\n",
      "Epoch 29/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 907us/step - accuracy: 0.8599 - loss: 0.4230 - val_accuracy: 0.8703 - val_loss: 0.4120\n",
      "Epoch 30/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 18s 904us/step - accuracy: 0.8618 - loss: 0.4182 - val_accuracy: 0.8599 - val_loss: 0.4185\n",
      "Epoch 31/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 908us/step - accuracy: 0.8645 - loss: 0.4112 - val_accuracy: 0.8506 - val_loss: 0.4637\n",
      "Epoch 32/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 908us/step - accuracy: 0.8642 - loss: 0.4109 - val_accuracy: 0.8709 - val_loss: 0.4094\n",
      "Epoch 33/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 905us/step - accuracy: 0.8668 - loss: 0.4028 - val_accuracy: 0.8665 - val_loss: 0.4050\n",
      "Epoch 34/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 905us/step - accuracy: 0.8678 - loss: 0.4020 - val_accuracy: 0.8808 - val_loss: 0.3750\n",
      "Epoch 35/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 910us/step - accuracy: 0.8689 - loss: 0.3953 - val_accuracy: 0.8711 - val_loss: 0.3922\n",
      "Epoch 36/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 907us/step - accuracy: 0.8713 - loss: 0.3905 - val_accuracy: 0.8669 - val_loss: 0.3918\n",
      "Epoch 37/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 911us/step - accuracy: 0.8714 - loss: 0.3894 - val_accuracy: 0.8771 - val_loss: 0.3776\n",
      "Epoch 38/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 910us/step - accuracy: 0.8733 - loss: 0.3850 - val_accuracy: 0.8745 - val_loss: 0.3830\n",
      "Epoch 39/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 916us/step - accuracy: 0.8739 - loss: 0.3843 - val_accuracy: 0.8792 - val_loss: 0.3773\n",
      "Epoch 40/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 19s 910us/step - accuracy: 0.8748 - loss: 0.3804 - val_accuracy: 0.8919 - val_loss: 0.3433\n",
      "40822/40822 ━━━━━━━━━━━━━━━━━━━━ 19s 457us/step\n",
      "10206/10206 ━━━━━━━━━━━━━━━━━━━━ 5s 500us/step\n",
      "12757/12757 ━━━━━━━━━━━━━━━━━━━━ 6s 495us/step\n",
      "Epoch 1/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 52s 2ms/step - accuracy: 0.8517 - loss: 0.5484 - val_accuracy: 0.9151 - val_loss: 0.2928\n",
      "Epoch 2/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 49s 2ms/step - accuracy: 0.9211 - loss: 0.2672 - val_accuracy: 0.9334 - val_loss: 0.2274\n",
      "Epoch 3/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 49s 2ms/step - accuracy: 0.9366 - loss: 0.2175 - val_accuracy: 0.9411 - val_loss: 0.2043\n",
      "Epoch 4/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 49s 2ms/step - accuracy: 0.9431 - loss: 0.1963 - val_accuracy: 0.9451 - val_loss: 0.1924\n",
      "Epoch 5/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9475 - loss: 0.1819 - val_accuracy: 0.9470 - val_loss: 0.1838\n",
      "Epoch 6/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9491 - loss: 0.1743 - val_accuracy: 0.9491 - val_loss: 0.1764\n",
      "Epoch 7/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9506 - loss: 0.1673 - val_accuracy: 0.9511 - val_loss: 0.1691\n",
      "Epoch 8/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9517 - loss: 0.1630 - val_accuracy: 0.9518 - val_loss: 0.1660\n",
      "Epoch 9/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9529 - loss: 0.1590 - val_accuracy: 0.9523 - val_loss: 0.1648\n",
      "Epoch 10/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 49s 2ms/step - accuracy: 0.9536 - loss: 0.1550 - val_accuracy: 0.9528 - val_loss: 0.1634\n",
      "Epoch 11/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9540 - loss: 0.1538 - val_accuracy: 0.9530 - val_loss: 0.1632\n",
      "Epoch 12/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9550 - loss: 0.1497 - val_accuracy: 0.9545 - val_loss: 0.1568\n",
      "Epoch 13/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9557 - loss: 0.1484 - val_accuracy: 0.9546 - val_loss: 0.1564\n",
      "Epoch 14/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9560 - loss: 0.1455 - val_accuracy: 0.9558 - val_loss: 0.1556\n",
      "Epoch 15/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9565 - loss: 0.1445 - val_accuracy: 0.9548 - val_loss: 0.1548\n",
      "Epoch 16/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9573 - loss: 0.1423 - val_accuracy: 0.9565 - val_loss: 0.1512\n",
      "Epoch 17/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9573 - loss: 0.1418 - val_accuracy: 0.9560 - val_loss: 0.1524\n",
      "Epoch 18/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9578 - loss: 0.1410 - val_accuracy: 0.9553 - val_loss: 0.1530\n",
      "Epoch 19/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9581 - loss: 0.1386 - val_accuracy: 0.9562 - val_loss: 0.1512\n",
      "Epoch 20/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9585 - loss: 0.1382 - val_accuracy: 0.9565 - val_loss: 0.1507\n",
      "Epoch 21/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9587 - loss: 0.1362 - val_accuracy: 0.9572 - val_loss: 0.1484\n",
      "Epoch 22/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9590 - loss: 0.1366 - val_accuracy: 0.9575 - val_loss: 0.1472\n",
      "Epoch 23/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9594 - loss: 0.1343 - val_accuracy: 0.9564 - val_loss: 0.1490\n",
      "Epoch 24/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9594 - loss: 0.1343 - val_accuracy: 0.9571 - val_loss: 0.1481\n",
      "Epoch 25/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9596 - loss: 0.1334 - val_accuracy: 0.9587 - val_loss: 0.1451\n",
      "Epoch 26/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9599 - loss: 0.1330 - val_accuracy: 0.9579 - val_loss: 0.1472\n",
      "Epoch 27/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9598 - loss: 0.1319 - val_accuracy: 0.9582 - val_loss: 0.1457\n",
      "Epoch 28/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9601 - loss: 0.1307 - val_accuracy: 0.9585 - val_loss: 0.1445\n",
      "Epoch 29/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9602 - loss: 0.1311 - val_accuracy: 0.9584 - val_loss: 0.1459\n",
      "Epoch 30/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9607 - loss: 0.1302 - val_accuracy: 0.9582 - val_loss: 0.1459\n",
      "Epoch 31/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9605 - loss: 0.1298 - val_accuracy: 0.9592 - val_loss: 0.1432\n",
      "Epoch 32/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9612 - loss: 0.1279 - val_accuracy: 0.9582 - val_loss: 0.1452\n",
      "Epoch 33/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9607 - loss: 0.1289 - val_accuracy: 0.9576 - val_loss: 0.1474\n",
      "Epoch 34/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9608 - loss: 0.1288 - val_accuracy: 0.9585 - val_loss: 0.1454\n",
      "Epoch 35/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9613 - loss: 0.1275 - val_accuracy: 0.9584 - val_loss: 0.1449\n",
      "Epoch 36/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 47s 2ms/step - accuracy: 0.9611 - loss: 0.1275 - val_accuracy: 0.9579 - val_loss: 0.1464\n",
      "Epoch 37/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9614 - loss: 0.1271 - val_accuracy: 0.9584 - val_loss: 0.1468\n",
      "Epoch 38/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9613 - loss: 0.1266 - val_accuracy: 0.9588 - val_loss: 0.1445\n",
      "Epoch 39/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9614 - loss: 0.1263 - val_accuracy: 0.9586 - val_loss: 0.1450\n",
      "Epoch 40/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 48s 2ms/step - accuracy: 0.9620 - loss: 0.1248 - val_accuracy: 0.9583 - val_loss: 0.1454\n",
      "10206/10206 ━━━━━━━━━━━━━━━━━━━━ 8s 802us/step - accuracy: 0.9579 - loss: 0.1473\n",
      "12757/12757 ━━━━━━━━━━━━━━━━━━━━ 10s 794us/step - accuracy: 0.9587 - loss: 0.1458\n",
      "Test Accuracy: 95.8581%\n",
      "10206/10206 ━━━━━━━━━━━━━━━━━━━━ 8s 736us/step\n",
      "12757/12757 ━━━━━━━━━━━━━━━━━━━━ 10s 760us/step\n",
      "Accuracy: 95.86\n",
      "Precision: 95.88\n",
      "Recall: 95.86\n",
      "F1-Score: 95.86\n",
      "Overall TP: 95.92\n",
      "Overall TN: 95.10\n",
      "Overall FP: 1.02\n",
      "Overall FN: 1.02\n"
     ]
    }
   ],
   "source": [
    "#CNN-LSTM-B\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-Update-3-All_together-Bi.csv',\n",
    "    '2-Update-3-All_together-Bi.csv',\n",
    "    '3-Update-3-All_together-Bi.csv',\n",
    "    \n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder_protocol = LabelEncoder()\n",
    "data['Protocol'] = label_encoder_protocol.fit_transform(data['Protocol'])\n",
    "\n",
    "label_encoder_label = LabelEncoder()\n",
    "data['Label'] = label_encoder_label.fit_transform(data['Label'])\n",
    "\n",
    "# Store the labels and drop unnecessary columns\n",
    "labels = data['Label'].values\n",
    "data.drop(columns=['Source IP', 'Destination IP', 'Label'], inplace=True)\n",
    "\n",
    "# Split the data into training (80%) and testing (20%) sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the 80% training data into training (80%) and validation (20%) sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the training, validation, and testing data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the data for Conv1D layers\n",
    "X_train_final = X_train_scaled.reshape((X_train_scaled.shape[0], X_train_scaled.shape[1], 1))\n",
    "X_val_final = X_val_scaled.reshape((X_val_scaled.shape[0], X_val_scaled.shape[1], 1))\n",
    "X_test_final = X_test_scaled.reshape((X_test_scaled.shape[0], X_test_scaled.shape[1], 1))\n",
    "\n",
    "# Define the CNN base model\n",
    "cnn_model = keras.Sequential([\n",
    "    layers.Input(shape=(X_train_final.shape[1], 1)),\n",
    "    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    layers.Conv1D(filters=64, kernel_size=3, activation='relu'),\n",
    "    layers.GlobalMaxPooling1D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(len(np.unique(labels)), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the CNN model\n",
    "cnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the CNN model with validation data\n",
    "cnn_model.fit(\n",
    "    X_train_final, y_train,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val_final, y_val)\n",
    ")\n",
    "\n",
    "# Generate predictions for stacking\n",
    "cnn_train_predictions = cnn_model.predict(X_train_final)\n",
    "cnn_val_predictions = cnn_model.predict(X_val_final)\n",
    "cnn_test_predictions = cnn_model.predict(X_test_final)\n",
    "\n",
    "# Use the predicted probabilities as features for the LSTM model\n",
    "cnn_train_preds_for_lstm = cnn_train_predictions.reshape(cnn_train_predictions.shape[0], cnn_train_predictions.shape[1], 1)\n",
    "cnn_val_preds_for_lstm = cnn_val_predictions.reshape(cnn_val_predictions.shape[0], cnn_val_predictions.shape[1], 1)\n",
    "cnn_test_preds_for_lstm = cnn_test_predictions.reshape(cnn_test_predictions.shape[0], cnn_test_predictions.shape[1], 1)\n",
    "\n",
    "# Define the LSTM meta model\n",
    "lstm_model = keras.Sequential([\n",
    "    layers.LSTM(64, input_shape=(cnn_train_preds_for_lstm.shape[1], cnn_train_preds_for_lstm.shape[2])),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(len(np.unique(labels)), activation='softmax')  # Output layer\n",
    "])\n",
    "\n",
    "# Compile the LSTM meta model\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the LSTM meta model with validation data\n",
    "lstm_model.fit(\n",
    "    cnn_train_preds_for_lstm, y_train,\n",
    "    epochs=40,\n",
    "    batch_size=64,\n",
    "    validation_data=(cnn_val_preds_for_lstm, y_val)\n",
    ")\n",
    "\n",
    "# Evaluate the LSTM model on the validation and test sets\n",
    "val_loss, val_accuracy = lstm_model.evaluate(cnn_val_preds_for_lstm, y_val)\n",
    "test_loss, test_accuracy = lstm_model.evaluate(cnn_test_preds_for_lstm, y_test)\n",
    "\n",
    "print(f'Validation Accuracy: {val_accuracy * 100:.4f}%')\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.4f}%')\n",
    "\n",
    "# Make predictions on the validation and test sets using the stacked model\n",
    "val_predictions = lstm_model.predict(cnn_val_preds_for_lstm)\n",
    "test_predictions = lstm_model.predict(cnn_test_preds_for_lstm)\n",
    "\n",
    "# Calculate predicted labels for validation and test sets\n",
    "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
    "test_predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Calculate metrics for test set\n",
    "test_accuracy = accuracy_score(y_test, test_predicted_labels)\n",
    "test_precision = precision_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "test_recall = recall_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "test_f1_score = f1_score(y_test, test_predicted_labels, average='weighted', zero_division=0)\n",
    "\n",
    "print(f'Accuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, test_predicted_labels)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec73d1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 530us/step - accuracy: 0.3120 - loss: 1.9937 - val_accuracy: 0.4350 - val_loss: 1.6618\n",
      "Epoch 2/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 527us/step - accuracy: 0.4553 - loss: 1.5939 - val_accuracy: 0.4958 - val_loss: 1.4838\n",
      "Epoch 3/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 536us/step - accuracy: 0.5038 - loss: 1.4525 - val_accuracy: 0.5279 - val_loss: 1.3923\n",
      "Epoch 4/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 537us/step - accuracy: 0.5398 - loss: 1.3617 - val_accuracy: 0.5465 - val_loss: 1.3551\n",
      "Epoch 5/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 19s 522us/step - accuracy: 0.5622 - loss: 1.2975 - val_accuracy: 0.5711 - val_loss: 1.2530\n",
      "Epoch 6/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 19s 524us/step - accuracy: 0.5797 - loss: 1.2462 - val_accuracy: 0.5790 - val_loss: 1.2399\n",
      "Epoch 7/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 19s 525us/step - accuracy: 0.5932 - loss: 1.2071 - val_accuracy: 0.5958 - val_loss: 1.2081\n",
      "Epoch 8/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 19s 527us/step - accuracy: 0.6044 - loss: 1.1745 - val_accuracy: 0.6140 - val_loss: 1.1446\n",
      "Epoch 9/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 528us/step - accuracy: 0.6144 - loss: 1.1435 - val_accuracy: 0.6151 - val_loss: 1.1348\n",
      "Epoch 10/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 535us/step - accuracy: 0.6228 - loss: 1.1180 - val_accuracy: 0.6329 - val_loss: 1.0938\n",
      "Epoch 11/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 535us/step - accuracy: 0.6302 - loss: 1.0963 - val_accuracy: 0.6300 - val_loss: 1.0982\n",
      "Epoch 12/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 538us/step - accuracy: 0.6356 - loss: 1.0818 - val_accuracy: 0.6536 - val_loss: 1.0546\n",
      "Epoch 13/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 536us/step - accuracy: 0.6415 - loss: 1.0652 - val_accuracy: 0.6461 - val_loss: 1.0465\n",
      "Epoch 14/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 532us/step - accuracy: 0.6466 - loss: 1.0489 - val_accuracy: 0.6508 - val_loss: 1.0403\n",
      "Epoch 15/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 538us/step - accuracy: 0.6507 - loss: 1.0368 - val_accuracy: 0.6556 - val_loss: 1.0181\n",
      "Epoch 16/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 536us/step - accuracy: 0.6563 - loss: 1.0227 - val_accuracy: 0.6641 - val_loss: 0.9964\n",
      "Epoch 17/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 540us/step - accuracy: 0.6607 - loss: 1.0095 - val_accuracy: 0.6482 - val_loss: 1.0337\n",
      "Epoch 18/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 19s 519us/step - accuracy: 0.6650 - loss: 0.9963 - val_accuracy: 0.6633 - val_loss: 1.0098\n",
      "Epoch 19/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 19s 501us/step - accuracy: 0.6688 - loss: 0.9864 - val_accuracy: 0.6635 - val_loss: 1.0018\n",
      "Epoch 20/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 544us/step - accuracy: 0.6721 - loss: 0.9751 - val_accuracy: 0.6734 - val_loss: 0.9623\n",
      "Epoch 21/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 543us/step - accuracy: 0.6761 - loss: 0.9628 - val_accuracy: 0.6852 - val_loss: 0.9353\n",
      "Epoch 22/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 542us/step - accuracy: 0.6808 - loss: 0.9499 - val_accuracy: 0.6886 - val_loss: 0.9261\n",
      "Epoch 23/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 541us/step - accuracy: 0.6838 - loss: 0.9409 - val_accuracy: 0.6891 - val_loss: 0.9382\n",
      "Epoch 24/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 544us/step - accuracy: 0.6877 - loss: 0.9305 - val_accuracy: 0.6854 - val_loss: 0.9453\n",
      "Epoch 25/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 546us/step - accuracy: 0.6901 - loss: 0.9235 - val_accuracy: 0.6750 - val_loss: 0.9598\n",
      "Epoch 26/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 546us/step - accuracy: 0.6933 - loss: 0.9136 - val_accuracy: 0.6993 - val_loss: 0.9050\n",
      "Epoch 27/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 544us/step - accuracy: 0.6962 - loss: 0.9067 - val_accuracy: 0.6837 - val_loss: 0.9356\n",
      "Epoch 28/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 544us/step - accuracy: 0.6989 - loss: 0.8983 - val_accuracy: 0.6979 - val_loss: 0.9091\n",
      "Epoch 29/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 545us/step - accuracy: 0.7014 - loss: 0.8937 - val_accuracy: 0.7012 - val_loss: 0.9001\n",
      "Epoch 30/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 546us/step - accuracy: 0.7040 - loss: 0.8877 - val_accuracy: 0.7028 - val_loss: 0.8857\n",
      "Epoch 31/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 545us/step - accuracy: 0.7068 - loss: 0.8782 - val_accuracy: 0.6997 - val_loss: 0.8992\n",
      "Epoch 32/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 542us/step - accuracy: 0.7103 - loss: 0.8694 - val_accuracy: 0.7060 - val_loss: 0.8753\n",
      "Epoch 33/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 544us/step - accuracy: 0.7112 - loss: 0.8659 - val_accuracy: 0.7124 - val_loss: 0.8679\n",
      "Epoch 34/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 546us/step - accuracy: 0.7141 - loss: 0.8595 - val_accuracy: 0.7112 - val_loss: 0.8601\n",
      "Epoch 35/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 548us/step - accuracy: 0.7163 - loss: 0.8525 - val_accuracy: 0.7142 - val_loss: 0.8608\n",
      "Epoch 36/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 545us/step - accuracy: 0.7173 - loss: 0.8496 - val_accuracy: 0.7295 - val_loss: 0.8128\n",
      "Epoch 37/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 547us/step - accuracy: 0.7198 - loss: 0.8425 - val_accuracy: 0.7136 - val_loss: 0.8452\n",
      "Epoch 38/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 546us/step - accuracy: 0.7216 - loss: 0.8377 - val_accuracy: 0.7082 - val_loss: 0.8667\n",
      "Epoch 39/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 545us/step - accuracy: 0.7232 - loss: 0.8317 - val_accuracy: 0.7111 - val_loss: 0.8705\n",
      "Epoch 40/40\n",
      "36906/36906 ━━━━━━━━━━━━━━━━━━━━ 20s 553us/step - accuracy: 0.7247 - loss: 0.8270 - val_accuracy: 0.7320 - val_loss: 0.7987\n",
      "73812/73812 ━━━━━━━━━━━━━━━━━━━━ 24s 328us/step\n",
      "31634/31634 ━━━━━━━━━━━━━━━━━━━━ 11s 349us/step\n",
      "26362/26362 ━━━━━━━━━━━━━━━━━━━━ 9s 351us/step\n",
      "\n",
      "Accuracy: 98.49\n",
      "Precision: 98.50\n",
      "Recall: 98.50\n",
      "F1-Score: 98.49\n",
      "Overall TP: 98.43\n",
      "Overall TN: 97.71\n",
      "Overall FP: 0.76\n",
      "Overall FN: 0.76\n"
     ]
    }
   ],
   "source": [
    "#FFNN-XGB-A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-All-Together-Update-Single-26-8-24.csv',\n",
    "    '2-All-Together-Update-Single-26-8-24.csv',\n",
    "    '3-All-Together-Update-Single-26-8-24.csv',\n",
    "    '4-All-Together-Update-Single-26-8-24.csv',\n",
    "    '5-All-Together-Update-Single-26-8-24.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['Source IP', 'Destination IP', 'Label'])\n",
    "y = data['Label']\n",
    "\n",
    "# Step 1: Split into Train, Validation, and Test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Initialize the scaler and fit it on the training, testing and validation data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the feedforward neural network model using Functional API\n",
    "inputs = keras.Input(shape=(X_train_scaled.shape[1],))\n",
    "x = layers.Dense(128, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)\n",
    "ffnn_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile and train the FFNN model\n",
    "ffnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "ffnn_model.fit(X_train_scaled, y_train, epochs=40, batch_size=64, validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# Extract intermediate output from the penultimate layer\n",
    "intermediate_layer_model = keras.Model(inputs=ffnn_model.input, outputs=ffnn_model.layers[-2].output)\n",
    "train_intermediate_output = intermediate_layer_model.predict(X_train_scaled)\n",
    "val_intermediate_output = intermediate_layer_model.predict(X_val_scaled)\n",
    "test_intermediate_output = intermediate_layer_model.predict(X_test_scaled)\n",
    "\n",
    "# Train XGBoost as the meta model on the FFNN's intermediate output\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y)), eval_metric='mlogloss')\n",
    "xgb_model.fit(train_intermediate_output, y_train)\n",
    "\n",
    "# --- Validation Metrics ---\n",
    "# Make predictions with the XGBoost model on validation set\n",
    "y_pred_val = xgb_model.predict(val_intermediate_output)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_precision = precision_score(y_val, y_pred_val, average='weighted')\n",
    "val_recall = recall_score(y_val, y_pred_val, average='weighted')\n",
    "val_f1_score = f1_score(y_val, y_pred_val, average='weighted')\n",
    "\n",
    "# --- Test Metrics ---\n",
    "# Make predictions with the XGBoost model on the test set\n",
    "y_pred_test = xgb_model.predict(test_intermediate_output)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_score = f1_score(y_test, y_pred_test, average='weighted')\n",
    "\n",
    "print(f'\\nAccuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f19f21f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 683us/step - accuracy: 0.2863 - loss: 2.0679 - val_accuracy: 0.3970 - val_loss: 1.7643\n",
      "Epoch 2/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 692us/step - accuracy: 0.4193 - loss: 1.7010 - val_accuracy: 0.4693 - val_loss: 1.5630\n",
      "Epoch 3/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 683us/step - accuracy: 0.4789 - loss: 1.5289 - val_accuracy: 0.5006 - val_loss: 1.4782\n",
      "Epoch 4/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 678us/step - accuracy: 0.5149 - loss: 1.4264 - val_accuracy: 0.5308 - val_loss: 1.3886\n",
      "Epoch 5/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 677us/step - accuracy: 0.5403 - loss: 1.3553 - val_accuracy: 0.5500 - val_loss: 1.3215\n",
      "Epoch 6/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 676us/step - accuracy: 0.5584 - loss: 1.3037 - val_accuracy: 0.5837 - val_loss: 1.2600\n",
      "Epoch 7/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 676us/step - accuracy: 0.5750 - loss: 1.2589 - val_accuracy: 0.5873 - val_loss: 1.2242\n",
      "Epoch 8/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 676us/step - accuracy: 0.5877 - loss: 1.2238 - val_accuracy: 0.5965 - val_loss: 1.2055\n",
      "Epoch 9/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 674us/step - accuracy: 0.5985 - loss: 1.1936 - val_accuracy: 0.5982 - val_loss: 1.1965\n",
      "Epoch 10/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 670us/step - accuracy: 0.6084 - loss: 1.1646 - val_accuracy: 0.6118 - val_loss: 1.1625\n",
      "Epoch 11/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 672us/step - accuracy: 0.6148 - loss: 1.1417 - val_accuracy: 0.6301 - val_loss: 1.1128\n",
      "Epoch 12/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 676us/step - accuracy: 0.6214 - loss: 1.1203 - val_accuracy: 0.6290 - val_loss: 1.1142\n",
      "Epoch 13/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 680us/step - accuracy: 0.6260 - loss: 1.1039 - val_accuracy: 0.6103 - val_loss: 1.1437\n",
      "Epoch 14/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 674us/step - accuracy: 0.6338 - loss: 1.0815 - val_accuracy: 0.6236 - val_loss: 1.0982\n",
      "Epoch 15/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 673us/step - accuracy: 0.6373 - loss: 1.0690 - val_accuracy: 0.6418 - val_loss: 1.0573\n",
      "Epoch 16/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 675us/step - accuracy: 0.6416 - loss: 1.0555 - val_accuracy: 0.6385 - val_loss: 1.0613\n",
      "Epoch 17/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 677us/step - accuracy: 0.6462 - loss: 1.0421 - val_accuracy: 0.6523 - val_loss: 1.0278\n",
      "Epoch 18/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 674us/step - accuracy: 0.6505 - loss: 1.0300 - val_accuracy: 0.6566 - val_loss: 1.0339\n",
      "Epoch 19/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 674us/step - accuracy: 0.6541 - loss: 1.0201 - val_accuracy: 0.6473 - val_loss: 1.0265\n",
      "Epoch 20/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 673us/step - accuracy: 0.6581 - loss: 1.0104 - val_accuracy: 0.6525 - val_loss: 1.0231\n",
      "Epoch 21/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 680us/step - accuracy: 0.6620 - loss: 0.9996 - val_accuracy: 0.6735 - val_loss: 0.9807\n",
      "Epoch 22/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 681us/step - accuracy: 0.6654 - loss: 0.9897 - val_accuracy: 0.6725 - val_loss: 0.9690\n",
      "Epoch 23/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 714us/step - accuracy: 0.6687 - loss: 0.9801 - val_accuracy: 0.6740 - val_loss: 0.9816\n",
      "Epoch 24/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 708us/step - accuracy: 0.6731 - loss: 0.9737 - val_accuracy: 0.6804 - val_loss: 0.9505\n",
      "Epoch 25/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 713us/step - accuracy: 0.6755 - loss: 0.9642 - val_accuracy: 0.6671 - val_loss: 0.9814\n",
      "Epoch 26/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 708us/step - accuracy: 0.6772 - loss: 0.9568 - val_accuracy: 0.6768 - val_loss: 0.9654\n",
      "Epoch 27/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 705us/step - accuracy: 0.6817 - loss: 0.9468 - val_accuracy: 0.6928 - val_loss: 0.9178\n",
      "Epoch 28/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 711us/step - accuracy: 0.6858 - loss: 0.9368 - val_accuracy: 0.7027 - val_loss: 0.8986\n",
      "Epoch 29/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 706us/step - accuracy: 0.6870 - loss: 0.9303 - val_accuracy: 0.6930 - val_loss: 0.9300\n",
      "Epoch 30/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 713us/step - accuracy: 0.6894 - loss: 0.9235 - val_accuracy: 0.6873 - val_loss: 0.9291\n",
      "Epoch 31/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 707us/step - accuracy: 0.6912 - loss: 0.9206 - val_accuracy: 0.6823 - val_loss: 0.9322\n",
      "Epoch 32/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 707us/step - accuracy: 0.6938 - loss: 0.9145 - val_accuracy: 0.6842 - val_loss: 0.9581\n",
      "Epoch 33/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 709us/step - accuracy: 0.6962 - loss: 0.9078 - val_accuracy: 0.6925 - val_loss: 0.9080\n",
      "Epoch 34/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 710us/step - accuracy: 0.6982 - loss: 0.9001 - val_accuracy: 0.7063 - val_loss: 0.8818\n",
      "Epoch 35/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 709us/step - accuracy: 0.6997 - loss: 0.8944 - val_accuracy: 0.7029 - val_loss: 0.9001\n",
      "Epoch 36/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 13s 703us/step - accuracy: 0.7018 - loss: 0.8896 - val_accuracy: 0.6994 - val_loss: 0.8956\n",
      "Epoch 37/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 694us/step - accuracy: 0.7030 - loss: 0.8861 - val_accuracy: 0.7008 - val_loss: 0.9009\n",
      "Epoch 38/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 690us/step - accuracy: 0.7050 - loss: 0.8809 - val_accuracy: 0.7224 - val_loss: 0.8532\n",
      "Epoch 39/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 692us/step - accuracy: 0.7071 - loss: 0.8772 - val_accuracy: 0.7033 - val_loss: 0.8876\n",
      "Epoch 40/40\n",
      "17860/17860 ━━━━━━━━━━━━━━━━━━━━ 12s 686us/step - accuracy: 0.7082 - loss: 0.8720 - val_accuracy: 0.7064 - val_loss: 0.8725\n",
      "35719/35719 ━━━━━━━━━━━━━━━━━━━━ 12s 343us/step\n",
      "15309/15309 ━━━━━━━━━━━━━━━━━━━━ 5s 351us/step\n",
      "12757/12757 ━━━━━━━━━━━━━━━━━━━━ 4s 350us/step\n",
      "\n",
      "Accuracy: 98.04\n",
      "Precision: 98.05\n",
      "Recall: 98.04\n",
      "F1-Score: 98.05\n",
      "Overall TP: 97.97\n",
      "Overall TN: 97.26\n",
      "Overall FP: 0.69\n",
      "Overall FN: 0.69\n"
     ]
    }
   ],
   "source": [
    "#FFNN-XGB-A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-Update-3-All_together-Bi.csv',\n",
    "    '2-Update-3-All_together-Bi.csv',\n",
    "    '3-Update-3-All_together-Bi.csv',\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['Source IP', 'Destination IP', 'Label'])\n",
    "y = data['Label']\n",
    "\n",
    "# Step 1: Split into Train, Validation, and Test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Initialize the scaler and fit it on the training, testing and validation data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define the feedforward neural network model using Functional API\n",
    "inputs = keras.Input(shape=(X_train_scaled.shape[1],))\n",
    "x = layers.Dense(128, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)\n",
    "ffnn_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# Compile and train the FFNN model\n",
    "ffnn_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "ffnn_model.fit(X_train_scaled, y_train, epochs=40, batch_size=64, validation_data=(X_val_scaled, y_val))\n",
    "\n",
    "# Extract intermediate output from the penultimate layer\n",
    "intermediate_layer_model = keras.Model(inputs=ffnn_model.input, outputs=ffnn_model.layers[-2].output)\n",
    "train_intermediate_output = intermediate_layer_model.predict(X_train_scaled)\n",
    "val_intermediate_output = intermediate_layer_model.predict(X_val_scaled)\n",
    "test_intermediate_output = intermediate_layer_model.predict(X_test_scaled)\n",
    "\n",
    "# Train XGBoost as the meta model on the FFNN's intermediate output\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y)), eval_metric='mlogloss')\n",
    "xgb_model.fit(train_intermediate_output, y_train)\n",
    "\n",
    "# --- Validation Metrics ---\n",
    "# Make predictions with the XGBoost model on validation set\n",
    "y_pred_val = xgb_model.predict(val_intermediate_output)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_precision = precision_score(y_val, y_pred_val, average='weighted')\n",
    "val_recall = recall_score(y_val, y_pred_val, average='weighted')\n",
    "val_f1_score = f1_score(y_val, y_pred_val, average='weighted')\n",
    "\n",
    "# --- Test Metrics ---\n",
    "# Make predictions with the XGBoost model on the test set\n",
    "y_pred_test = xgb_model.predict(test_intermediate_output)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_score = f1_score(y_test, y_pred_test, average='weighted')\n",
    "\n",
    "print(f'\\nAccuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74b5bbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.4002 - loss: 1.7764 - val_accuracy: 0.6400 - val_loss: 1.1161\n",
      "Epoch 2/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 49s 1ms/step - accuracy: 0.6648 - loss: 1.0372 - val_accuracy: 0.7289 - val_loss: 0.8667\n",
      "Epoch 3/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 49s 1ms/step - accuracy: 0.7344 - loss: 0.8348 - val_accuracy: 0.7621 - val_loss: 0.7387\n",
      "Epoch 4/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 50s 1ms/step - accuracy: 0.7714 - loss: 0.7162 - val_accuracy: 0.7954 - val_loss: 0.6497\n",
      "Epoch 5/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 50s 1ms/step - accuracy: 0.7976 - loss: 0.6381 - val_accuracy: 0.8084 - val_loss: 0.6032\n",
      "Epoch 6/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 50s 1ms/step - accuracy: 0.8169 - loss: 0.5820 - val_accuracy: 0.8261 - val_loss: 0.5520\n",
      "Epoch 7/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 50s 1ms/step - accuracy: 0.8306 - loss: 0.5404 - val_accuracy: 0.8410 - val_loss: 0.5081\n",
      "Epoch 8/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 50s 1ms/step - accuracy: 0.8432 - loss: 0.5031 - val_accuracy: 0.8514 - val_loss: 0.4785\n",
      "Epoch 9/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 50s 1ms/step - accuracy: 0.8526 - loss: 0.4761 - val_accuracy: 0.8628 - val_loss: 0.4420\n",
      "Epoch 10/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 50s 1ms/step - accuracy: 0.8616 - loss: 0.4500 - val_accuracy: 0.8646 - val_loss: 0.4393\n",
      "Epoch 11/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 50s 1ms/step - accuracy: 0.8685 - loss: 0.4301 - val_accuracy: 0.8632 - val_loss: 0.4572\n",
      "Epoch 12/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 49s 1ms/step - accuracy: 0.8737 - loss: 0.4147 - val_accuracy: 0.8690 - val_loss: 0.4193\n",
      "Epoch 13/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 55s 1ms/step - accuracy: 0.8788 - loss: 0.3991 - val_accuracy: 0.8788 - val_loss: 0.4020\n",
      "Epoch 14/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 66s 2ms/step - accuracy: 0.8826 - loss: 0.3880 - val_accuracy: 0.8925 - val_loss: 0.3656\n",
      "Epoch 15/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.8864 - loss: 0.3757 - val_accuracy: 0.8858 - val_loss: 0.3736\n",
      "Epoch 16/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.8887 - loss: 0.3672 - val_accuracy: 0.8925 - val_loss: 0.3541\n",
      "Epoch 17/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.8917 - loss: 0.3578 - val_accuracy: 0.8943 - val_loss: 0.3419\n",
      "Epoch 18/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.8939 - loss: 0.3475 - val_accuracy: 0.8993 - val_loss: 0.3315\n",
      "Epoch 19/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.8962 - loss: 0.3430 - val_accuracy: 0.9011 - val_loss: 0.3251\n",
      "Epoch 20/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 50s 1ms/step - accuracy: 0.8984 - loss: 0.3332 - val_accuracy: 0.8961 - val_loss: 0.3379\n",
      "Epoch 21/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.8995 - loss: 0.3299 - val_accuracy: 0.9053 - val_loss: 0.3132\n",
      "Epoch 22/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9014 - loss: 0.3247 - val_accuracy: 0.9050 - val_loss: 0.3123\n",
      "Epoch 23/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9030 - loss: 0.3187 - val_accuracy: 0.9025 - val_loss: 0.3116\n",
      "Epoch 24/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9053 - loss: 0.3137 - val_accuracy: 0.9102 - val_loss: 0.2934\n",
      "Epoch 25/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 52s 1ms/step - accuracy: 0.9065 - loss: 0.3084 - val_accuracy: 0.9077 - val_loss: 0.3047\n",
      "Epoch 26/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9079 - loss: 0.3058 - val_accuracy: 0.9088 - val_loss: 0.2890\n",
      "Epoch 27/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9097 - loss: 0.2996 - val_accuracy: 0.9084 - val_loss: 0.3009\n",
      "Epoch 28/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9108 - loss: 0.2960 - val_accuracy: 0.8948 - val_loss: 0.3465\n",
      "Epoch 29/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9115 - loss: 0.2954 - val_accuracy: 0.9105 - val_loss: 0.3011\n",
      "Epoch 30/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9131 - loss: 0.2900 - val_accuracy: 0.9133 - val_loss: 0.2813\n",
      "Epoch 31/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9144 - loss: 0.2866 - val_accuracy: 0.9135 - val_loss: 0.2943\n",
      "Epoch 32/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9155 - loss: 0.2835 - val_accuracy: 0.9145 - val_loss: 0.2834\n",
      "Epoch 33/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9164 - loss: 0.2812 - val_accuracy: 0.9216 - val_loss: 0.2640\n",
      "Epoch 34/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 50s 1ms/step - accuracy: 0.9180 - loss: 0.2776 - val_accuracy: 0.9222 - val_loss: 0.2618\n",
      "Epoch 35/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9194 - loss: 0.2721 - val_accuracy: 0.9235 - val_loss: 0.2558\n",
      "Epoch 36/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 52s 1ms/step - accuracy: 0.9207 - loss: 0.2675 - val_accuracy: 0.9184 - val_loss: 0.2774\n",
      "Epoch 37/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 52s 1ms/step - accuracy: 0.9215 - loss: 0.2660 - val_accuracy: 0.9220 - val_loss: 0.2682\n",
      "Epoch 38/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9225 - loss: 0.2622 - val_accuracy: 0.9244 - val_loss: 0.2547\n",
      "Epoch 39/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9231 - loss: 0.2623 - val_accuracy: 0.9227 - val_loss: 0.2597\n",
      "Epoch 40/40\n",
      "42178/42178 ━━━━━━━━━━━━━━━━━━━━ 51s 1ms/step - accuracy: 0.9234 - loss: 0.2597 - val_accuracy: 0.9228 - val_loss: 0.2643\n",
      "84356/84356 ━━━━━━━━━━━━━━━━━━━━ 42s 495us/step\n",
      "21089/21089 ━━━━━━━━━━━━━━━━━━━━ 12s 572us/step\n",
      "26362/26362 ━━━━━━━━━━━━━━━━━━━━ 15s 582us/step\n",
      "\n",
      "Accuracy: 99.03\n",
      "Precision: 99.03\n",
      "Recall: 99.03\n",
      "F1-Score: 99.03\n",
      "Overall TP: 99.01\n",
      "Overall TN: 98.31\n",
      "Overall FP: 0.49\n",
      "Overall FN: 0.49\n"
     ]
    }
   ],
   "source": [
    "#LSTM-XGB-A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-All-Together-Update-Single-26-8-24.csv',\n",
    "    '2-All-Together-Update-Single-26-8-24.csv',\n",
    "    '3-All-Together-Update-Single-26-8-24.csv',\n",
    "    '4-All-Together-Update-Single-26-8-24.csv',\n",
    "    '5-All-Together-Update-Single-26-8-24.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['Source IP', 'Destination IP', 'Label'])\n",
    "y = data['Label']\n",
    "\n",
    "# Step 1: Split into Train, Validation, and Test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Initialize the scaler and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Reshape the data for LSTM input (LSTM expects 3D input: [samples, timesteps, features])\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# Step 4: Define the LSTM model using Functional API\n",
    "inputs = layers.Input(shape=(1, X_train_reshaped.shape[2]))\n",
    "x = layers.LSTM(128, activation='tanh', return_sequences=True)(inputs)\n",
    "x = layers.LSTM(64, activation='tanh')(x)\n",
    "outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)\n",
    "lstm_model = Model(inputs, outputs)\n",
    "\n",
    "# Compile and train the LSTM model\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train_reshaped, y_train, epochs=40, batch_size=64, validation_data=(X_val_reshaped, y_val))\n",
    "\n",
    "# Extract intermediate output from the penultimate layer\n",
    "intermediate_layer_model = Model(inputs=lstm_model.input, outputs=lstm_model.layers[-2].output)\n",
    "train_intermediate_output = intermediate_layer_model.predict(X_train_reshaped)\n",
    "val_intermediate_output = intermediate_layer_model.predict(X_val_reshaped)\n",
    "test_intermediate_output = intermediate_layer_model.predict(X_test_reshaped)\n",
    "\n",
    "# Train XGBoost as the meta model on the LSTM's intermediate output\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y)), eval_metric='mlogloss')\n",
    "xgb_model.fit(train_intermediate_output, y_train)\n",
    "\n",
    "# --- Validation Metrics ---\n",
    "# Make predictions with the XGBoost model on validation set\n",
    "y_pred_val = xgb_model.predict(val_intermediate_output)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_precision = precision_score(y_val, y_pred_val, average='weighted')\n",
    "val_recall = recall_score(y_val, y_pred_val, average='weighted')\n",
    "val_f1_score = f1_score(y_val, y_pred_val, average='weighted')\n",
    "\n",
    "# --- Test Metrics ---\n",
    "# Make predictions with the XGBoost model on the test set\n",
    "y_pred_test = xgb_model.predict(test_intermediate_output)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_score = f1_score(y_test, y_pred_test, average='weighted')\n",
    "\n",
    "print(f'\\nAccuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b66314c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 36s 2ms/step - accuracy: 0.3310 - loss: 1.9546 - val_accuracy: 0.5299 - val_loss: 1.4124\n",
      "Epoch 2/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 34s 2ms/step - accuracy: 0.5591 - loss: 1.3175 - val_accuracy: 0.6262 - val_loss: 1.1161\n",
      "Epoch 3/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.6463 - loss: 1.0737 - val_accuracy: 0.6853 - val_loss: 0.9717\n",
      "Epoch 4/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 34s 2ms/step - accuracy: 0.6943 - loss: 0.9423 - val_accuracy: 0.7156 - val_loss: 0.8838\n",
      "Epoch 5/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.7252 - loss: 0.8528 - val_accuracy: 0.7427 - val_loss: 0.8022\n",
      "Epoch 6/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 34s 2ms/step - accuracy: 0.7462 - loss: 0.7911 - val_accuracy: 0.7588 - val_loss: 0.7547\n",
      "Epoch 7/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.7616 - loss: 0.7412 - val_accuracy: 0.7739 - val_loss: 0.7082\n",
      "Epoch 8/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.7749 - loss: 0.7013 - val_accuracy: 0.7841 - val_loss: 0.6791\n",
      "Epoch 9/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.7870 - loss: 0.6663 - val_accuracy: 0.7871 - val_loss: 0.6750\n",
      "Epoch 10/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.7969 - loss: 0.6359 - val_accuracy: 0.8079 - val_loss: 0.6074\n",
      "Epoch 11/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8057 - loss: 0.6079 - val_accuracy: 0.8101 - val_loss: 0.5984\n",
      "Epoch 12/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8120 - loss: 0.5868 - val_accuracy: 0.8210 - val_loss: 0.5678\n",
      "Epoch 13/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8206 - loss: 0.5636 - val_accuracy: 0.8290 - val_loss: 0.5378\n",
      "Epoch 14/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8285 - loss: 0.5422 - val_accuracy: 0.8241 - val_loss: 0.5438\n",
      "Epoch 15/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8340 - loss: 0.5236 - val_accuracy: 0.8416 - val_loss: 0.5082\n",
      "Epoch 16/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8395 - loss: 0.5079 - val_accuracy: 0.8413 - val_loss: 0.5009\n",
      "Epoch 17/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8435 - loss: 0.4922 - val_accuracy: 0.8480 - val_loss: 0.4832\n",
      "Epoch 18/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8470 - loss: 0.4816 - val_accuracy: 0.8508 - val_loss: 0.4739\n",
      "Epoch 19/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8506 - loss: 0.4700 - val_accuracy: 0.8523 - val_loss: 0.4690\n",
      "Epoch 20/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8549 - loss: 0.4565 - val_accuracy: 0.8540 - val_loss: 0.4527\n",
      "Epoch 21/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8580 - loss: 0.4470 - val_accuracy: 0.8655 - val_loss: 0.4256\n",
      "Epoch 22/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8614 - loss: 0.4355 - val_accuracy: 0.8636 - val_loss: 0.4294\n",
      "Epoch 23/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8630 - loss: 0.4317 - val_accuracy: 0.8660 - val_loss: 0.4278\n",
      "Epoch 24/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8663 - loss: 0.4222 - val_accuracy: 0.8666 - val_loss: 0.4279\n",
      "Epoch 25/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8682 - loss: 0.4168 - val_accuracy: 0.8756 - val_loss: 0.3954\n",
      "Epoch 26/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8714 - loss: 0.4061 - val_accuracy: 0.8721 - val_loss: 0.4066\n",
      "Epoch 27/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8737 - loss: 0.4008 - val_accuracy: 0.8767 - val_loss: 0.3987\n",
      "Epoch 28/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8762 - loss: 0.3926 - val_accuracy: 0.8783 - val_loss: 0.3901\n",
      "Epoch 29/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8774 - loss: 0.3879 - val_accuracy: 0.8805 - val_loss: 0.3835\n",
      "Epoch 30/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8792 - loss: 0.3834 - val_accuracy: 0.8761 - val_loss: 0.3935\n",
      "Epoch 31/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8799 - loss: 0.3819 - val_accuracy: 0.8859 - val_loss: 0.3664\n",
      "Epoch 32/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8820 - loss: 0.3743 - val_accuracy: 0.8819 - val_loss: 0.3789\n",
      "Epoch 33/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8845 - loss: 0.3685 - val_accuracy: 0.8842 - val_loss: 0.3637\n",
      "Epoch 34/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8861 - loss: 0.3625 - val_accuracy: 0.8865 - val_loss: 0.3633\n",
      "Epoch 35/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8876 - loss: 0.3583 - val_accuracy: 0.8901 - val_loss: 0.3556\n",
      "Epoch 36/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8898 - loss: 0.3539 - val_accuracy: 0.8884 - val_loss: 0.3540\n",
      "Epoch 37/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8901 - loss: 0.3524 - val_accuracy: 0.8915 - val_loss: 0.3466\n",
      "Epoch 38/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8913 - loss: 0.3506 - val_accuracy: 0.8905 - val_loss: 0.3553\n",
      "Epoch 39/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8920 - loss: 0.3453 - val_accuracy: 0.8863 - val_loss: 0.3724\n",
      "Epoch 40/40\n",
      "20411/20411 ━━━━━━━━━━━━━━━━━━━━ 33s 2ms/step - accuracy: 0.8934 - loss: 0.3422 - val_accuracy: 0.8968 - val_loss: 0.3381\n",
      "40822/40822 ━━━━━━━━━━━━━━━━━━━━ 21s 520us/step\n",
      "10206/10206 ━━━━━━━━━━━━━━━━━━━━ 6s 541us/step\n",
      "12757/12757 ━━━━━━━━━━━━━━━━━━━━ 7s 555us/step\n",
      "\n",
      "Accuracy: 99.04\n",
      "Precision: 99.04\n",
      "Recall: 99.04\n",
      "F1-Score: 99.04\n",
      "Overall TP: 99.24\n",
      "Overall TN: 98.68\n",
      "Overall FP: 0.29\n",
      "Overall FN: 0.29\n"
     ]
    }
   ],
   "source": [
    "#LSTM-XGB-A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the dataset\n",
    "filenames = [\n",
    "    '1-Update-3-All_together-Bi.csv',\n",
    "    '2-Update-3-All_together-Bi.csv',\n",
    "    '3-Update-3-All_together-Bi.csv'\n",
    "]\n",
    "\n",
    "# Concatenate all chunks into a single DataFrame\n",
    "dataframes = [pd.read_csv(filename) for filename in filenames]\n",
    "data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Apply Label Encoding to the \"Protocol\" and \"Label\" columns\n",
    "label_encoder = LabelEncoder()\n",
    "data['Protocol'] = label_encoder.fit_transform(data['Protocol'])\n",
    "data['Label'] = label_encoder.fit_transform(data['Label'])\n",
    "\n",
    "# Separate features and target\n",
    "X = data.drop(columns=['Source IP', 'Destination IP', 'Label'])\n",
    "y = data['Label']\n",
    "\n",
    "# Step 1: Split into Train, Validation, and Test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Initialize the scaler and fit it on the training data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 3: Reshape the data for LSTM input (LSTM expects 3D input: [samples, timesteps, features])\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "X_val_reshaped = X_val_scaled.reshape((X_val_scaled.shape[0], 1, X_val_scaled.shape[1]))\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# Step 4: Define the LSTM model using Functional API\n",
    "inputs = layers.Input(shape=(1, X_train_reshaped.shape[2]))\n",
    "x = layers.LSTM(128, activation='tanh', return_sequences=True)(inputs)\n",
    "x = layers.LSTM(64, activation='tanh')(x)\n",
    "outputs = layers.Dense(len(np.unique(y)), activation='softmax')(x)\n",
    "lstm_model = Model(inputs, outputs)\n",
    "\n",
    "# Compile and train the LSTM model\n",
    "lstm_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "lstm_model.fit(X_train_reshaped, y_train, epochs=40, batch_size=64, validation_data=(X_val_reshaped, y_val))\n",
    "\n",
    "# Extract intermediate output from the penultimate layer\n",
    "intermediate_layer_model = Model(inputs=lstm_model.input, outputs=lstm_model.layers[-2].output)\n",
    "train_intermediate_output = intermediate_layer_model.predict(X_train_reshaped)\n",
    "val_intermediate_output = intermediate_layer_model.predict(X_val_reshaped)\n",
    "test_intermediate_output = intermediate_layer_model.predict(X_test_reshaped)\n",
    "\n",
    "# Train XGBoost as the meta model on the LSTM's intermediate output\n",
    "xgb_model = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y)), eval_metric='mlogloss')\n",
    "xgb_model.fit(train_intermediate_output, y_train)\n",
    "\n",
    "# --- Validation Metrics ---\n",
    "# Make predictions with the XGBoost model on validation set\n",
    "y_pred_val = xgb_model.predict(val_intermediate_output)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the validation set\n",
    "val_accuracy = accuracy_score(y_val, y_pred_val)\n",
    "val_precision = precision_score(y_val, y_pred_val, average='weighted')\n",
    "val_recall = recall_score(y_val, y_pred_val, average='weighted')\n",
    "val_f1_score = f1_score(y_val, y_pred_val, average='weighted')\n",
    "\n",
    "# --- Test Metrics ---\n",
    "# Make predictions with the XGBoost model on the test set\n",
    "y_pred_test = xgb_model.predict(test_intermediate_output)\n",
    "\n",
    "# Calculate accuracy, precision, recall, and F1-score on the test set\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "test_precision = precision_score(y_test, y_pred_test, average='weighted')\n",
    "test_recall = recall_score(y_test, y_pred_test, average='weighted')\n",
    "test_f1_score = f1_score(y_test, y_pred_test, average='weighted')\n",
    "\n",
    "print(f'\\nAccuracy: {test_accuracy * 100:.2f}%')\n",
    "print(f'Precision: {test_precision:.2f}')\n",
    "print(f'Recall: {test_recall:.2f}')\n",
    "print(f'F1-Score: {test_f1_score:.2f}')\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "\n",
    "# Aggregate TP, TN, FP, FN for all classes\n",
    "TP = np.sum(np.diag(conf_matrix))  # True Positives\n",
    "FP = np.sum(np.sum(conf_matrix, axis=0) - np.diag(conf_matrix))  # False Positives\n",
    "FN = np.sum(np.sum(conf_matrix, axis=1) - np.diag(conf_matrix))  # False Negatives\n",
    "TN = np.sum(conf_matrix) - (TP + FP + FN)  # True Negatives\n",
    "\n",
    "# Calculate percentages\n",
    "total_instances = np.sum(conf_matrix)\n",
    "TP_percentage = (TP / total_instances) * 100\n",
    "FP_percentage = (FP / total_instances) * 100\n",
    "FN_percentage = (FN / total_instances) * 100\n",
    "TN_percentage = (TN / total_instances) * 100\n",
    "\n",
    "# Print TP, TN, FP, FN as percentages\n",
    "print(f\"Overall TP: {TP} ({TP_percentage:.2f}%)\")\n",
    "print(f\"Overall TN: {TN} ({TN_percentage:.2f}%)\")\n",
    "print(f\"Overall FP: {FP} ({FP_percentage:.2f}%)\")\n",
    "print(f\"Overall FN: {FN} ({FN_percentage:.2f}%)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce0e67e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
